#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDF è™•ç†å™¨æœå‹™ - å« OCR åŠŸèƒ½
å°ˆæ³¨æ–¼å°‡ PDF æƒæä¸¦å­˜å…¥ technical_documents ä¸»è¡¨
æ”¯æ´æ–‡å­—å‹å’Œæƒæå‹ PDF
"""

import os
import sys
import time
import json
import hashlib
import re
import pymysql
import pdfplumber
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import logging

# OCR ç›¸é—œ
try:
    from pdf2image import convert_from_path
    import pytesseract
    from PIL import Image
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    logging.warning("OCR å¥—ä»¶æœªå®‰è£ï¼Œå°‡ç„¡æ³•è™•ç†æƒæç‰ˆ PDF")

# ========== é…ç½® ==========
MYSQL_HOST = os.environ.get('MYSQL_HOST', 'mysql')
MYSQL_PORT = int(os.environ.get('MYSQL_PORT', '3306'))
MYSQL_USER = os.environ.get('MYSQL_USER', 'root')
MYSQL_PASSWORD = os.environ.get('MYSQL_PASSWORD', 'root')
MYSQL_DATABASE = os.environ.get('MYSQL_DATABASE', 'fuhsin_erp_demo')

PDF_WATCH_DIR = Path(os.environ.get('PDF_WATCH_DIR', '/mnt/pdf/incoming'))
PDF_PROCESSING_DIR = Path(os.environ.get('PDF_PROCESSING_DIR', '/mnt/pdf/processing'))
PDF_DONE_DIR = Path(os.environ.get('PDF_DONE_DIR', '/mnt/pdf/done'))
PDF_ERROR_DIR = Path(os.environ.get('PDF_ERROR_DIR', '/mnt/pdf/error'))

SCAN_INTERVAL = int(os.environ.get('SCAN_INTERVAL', '20'))
BATCH_SIZE = int(os.environ.get('PROCESS_BATCH_SIZE', '3'))

# OCR é…ç½®
ENABLE_OCR = os.environ.get('ENABLE_OCR', 'true').lower() == 'true'
OCR_LANG = os.environ.get('OCR_LANG', 'chi_tra+eng')  # ç¹é«”ä¸­æ–‡+è‹±æ–‡
OCR_DPI = int(os.environ.get('OCR_DPI', '300'))  # OCR è§£æåº¦

# ========== æ—¥èªŒé…ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ========== è³‡æ–™æ¨¡å‹ ==========
@dataclass
class TechnicalDocument:
    """ç°¡åŒ–çš„æŠ€è¡“æ–‡ä»¶æ¨¡å‹"""
    doc_id: str
    doc_type: str
    file_name: str
    file_size: int
    page_count: int
    content: str
    created_at: datetime

# ========== è³‡æ–™åº«ç®¡ç†å™¨ ==========
class DatabaseManager:
    """è³‡æ–™åº«æ“ä½œç®¡ç†å™¨"""
    
    def __init__(self):
        self.connection = None
        self.connect()
        self.init_database()

    def connect(self):
        """å»ºç«‹è³‡æ–™åº«é€£ç·š"""
        try:
            self.connection = pymysql.connect(
                host=MYSQL_HOST,
                port=MYSQL_PORT,
                user=MYSQL_USER,
                password=MYSQL_PASSWORD,
                database=MYSQL_DATABASE,
                charset='utf8mb4',
                cursorclass=pymysql.cursors.DictCursor,
                autocommit=False
            )
            logger.info(f"âœ… è³‡æ–™åº«é€£ç·šæˆåŠŸ: {MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}")
        except Exception as e:
            logger.error(f"âŒ è³‡æ–™åº«é€£ç·šå¤±æ•—: {e}")
            raise
    
    def init_database(self):
        """åˆå§‹åŒ–è³‡æ–™åº«è¡¨æ ¼ - åªå»ºç«‹ä¸»è¡¨"""
        try:
            conn = self.get_connection()
            with conn.cursor() as cursor:
                # å»ºç«‹ technical_documents ä¸»è¡¨
                cursor.execute("""
                CREATE TABLE IF NOT EXISTS technical_documents (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    doc_id VARCHAR(32) UNIQUE NOT NULL,
                    doc_type VARCHAR(50),
                    file_name VARCHAR(255) NOT NULL,
                    file_size INT,
                    page_count INT,
                    content LONGTEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    last_modified TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
                    INDEX idx_doc_type (doc_type),
                    INDEX idx_created_at (created_at),
                    FULLTEXT idx_content (content)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                """)
                
                # å»ºç«‹ pdf_processing_log è¡¨
                cursor.execute("""
                CREATE TABLE IF NOT EXISTS pdf_processing_log (
                    id INT AUTO_INCREMENT PRIMARY KEY,
                    file_name VARCHAR(255) NOT NULL,
                    file_hash VARCHAR(32),
                    status ENUM('processing', 'success', 'error', 'skipped') NOT NULL,
                    error_message TEXT,
                    process_time_ms INT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    INDEX idx_status (status),
                    INDEX idx_created_at (created_at)
                ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
                """)
                
                conn.commit()
                logger.info("âœ… ç¢ºèªè¡¨æ ¼å­˜åœ¨: technical_documents, pdf_processing_log")
                
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–è³‡æ–™åº«å¤±æ•—: {e}")

    def get_connection(self):
        """å–å¾—è³‡æ–™åº«é€£ç·šï¼Œå¿…è¦æ™‚é‡æ–°é€£ç·š"""
        try:
            if not self.connection or not self.connection.ping(reconnect=False):
                logger.warning("è³‡æ–™åº«é€£ç·šå·²æ–·é–‹ï¼Œå˜—è©¦é‡æ–°é€£ç·š...")
                self.connect()
        except:
            self.connect()
        return self.connection

    def save_document(self, doc: TechnicalDocument) -> bool:
        """å„²å­˜æ–‡æª”åˆ°è³‡æ–™åº« - ç°¡åŒ–ç‰ˆ"""
        sql = """
        INSERT INTO technical_documents 
        (doc_id, doc_type, file_name, file_size, page_count, content)
        VALUES (%(doc_id)s, %(doc_type)s, %(file_name)s, %(file_size)s, %(page_count)s, %(content)s)
        ON DUPLICATE KEY UPDATE
            doc_type = VALUES(doc_type),
            file_name = VALUES(file_name),
            file_size = VALUES(file_size),
            page_count = VALUES(page_count),
            content = VALUES(content)
        """
        
        try:
            conn = self.get_connection()
            with conn.cursor() as cursor:
                cursor.execute(sql, {
                    'doc_id': doc.doc_id,
                    'doc_type': doc.doc_type,
                    'file_name': doc.file_name,
                    'file_size': doc.file_size,
                    'page_count': doc.page_count,
                    'content': doc.content
                })
                conn.commit()
                logger.info(f"âœ… æ–‡æª”å·²å„²å­˜: {doc.doc_id}")
                return True
        except Exception as e:
            logger.error(f"å„²å­˜æ–‡æª”å¤±æ•— {doc.doc_id}: {e}")
            return False
    
    def check_document_exists(self, doc_id: str) -> bool:
        """æª¢æŸ¥æ–‡æª”æ˜¯å¦å·²å­˜åœ¨"""
        try:
            conn = self.get_connection()
            with conn.cursor() as cursor:
                cursor.execute(
                    "SELECT 1 FROM technical_documents WHERE doc_id = %s",
                    (doc_id,)
                )
                return cursor.fetchone() is not None
        except Exception as e:
            logger.error(f"æª¢æŸ¥æ–‡æª”å­˜åœ¨æ€§å¤±æ•—: {e}")
            return False
    
    def log_processing(self, file_name: str, file_hash: str, status: str,
                        error_message: str = None, process_time_ms: int = 0):
        """è¨˜éŒ„è™•ç†ç‹€æ…‹åˆ°æ—¥èªŒè¡¨"""
        sql = """
        INSERT INTO pdf_processing_log 
        (file_name, file_hash, status, error_message, process_time_ms)
        VALUES (%s, %s, %s, %s, %s)
        """
        try:
            conn = self.get_connection()
            with conn.cursor() as cursor:
                cursor.execute(sql, (file_name, file_hash, status, error_message, process_time_ms))
                conn.commit()
        except Exception as e:
            logger.error(f"è¨˜éŒ„è™•ç†æ—¥èªŒå¤±æ•—: {e}")

    def close(self):
        """é—œé–‰è³‡æ–™åº«é€£ç·š"""
        if self.connection:
            self.connection.close()
            logger.info("è³‡æ–™åº«é€£ç·šå·²é—œé–‰")

# ========== PDF è§£æå™¨ ==========
class SimplePDFParser:
    """ç°¡åŒ–çš„ PDF è§£æå™¨ - æ”¯æ´ OCR"""
    
    @staticmethod
    def extract_text(pdf_path: Path) -> tuple[str, int]:
        """æå– PDF æ–‡å­—å…§å®¹ - å„ªå…ˆä½¿ç”¨æ–‡å­—æå–ï¼Œå¤±æ•—å‰‡ä½¿ç”¨ OCR"""
        text_parts = []
        page_count = 0
        
        try:
            # æ–¹æ³• 1: ä½¿ç”¨ pdfplumber æå–æ–‡å­—
            logger.info(f"  å˜—è©¦ä½¿ç”¨ pdfplumber æå–æ–‡å­—...")
            with pdfplumber.open(pdf_path) as pdf:
                for page_num, page in enumerate(pdf.pages):
                    page_count += 1
                    page_text = page.extract_text() or ""
                    
                    if page_text and len(page_text.strip()) > 50:
                        text_parts.append(f"[ç¬¬ {page_num + 1} é ]\n{page_text}")
            
            combined_text = '\n'.join(text_parts)
            
            # æª¢æŸ¥æå–çµæœ
            if combined_text and len(combined_text.strip()) > 100:
                logger.info(f"  âœ… pdfplumber æå–æˆåŠŸ: {len(combined_text)} å­—å…ƒ")
                return combined_text, page_count
            
            # æ–¹æ³• 2: å¦‚æœæ–‡å­—æå–å¤±æ•—ï¼Œä½¿ç”¨ OCR
            if ENABLE_OCR and OCR_AVAILABLE:
                logger.warning(f"  æ–‡å­—æå–çµæœä¸è¶³ï¼Œå˜—è©¦ä½¿ç”¨ OCR...")
                ocr_text, ocr_pages = SimplePDFParser.extract_text_with_ocr(pdf_path)
                
                if ocr_text and len(ocr_text.strip()) > 100:
                    logger.info(f"  âœ… OCR æå–æˆåŠŸ: {len(ocr_text)} å­—å…ƒ")
                    return ocr_text, ocr_pages
                else:
                    logger.error(f"  âŒ OCR æå–å¤±æ•—æˆ–çµæœä¸è¶³")
            elif ENABLE_OCR and not OCR_AVAILABLE:
                logger.error(f"  âŒ OCR å·²å•Ÿç”¨ä½†ç›¸é—œå¥—ä»¶æœªå®‰è£")
            else:
                logger.warning(f"  OCR æœªå•Ÿç”¨ï¼Œè·³é")
            
            return combined_text, page_count
            
        except Exception as e:
            logger.error(f"  æå–æ–‡å­—å¤±æ•—: {e}")
            
            # æœ€å¾Œå˜—è©¦ï¼šä½¿ç”¨ OCR
            if ENABLE_OCR and OCR_AVAILABLE:
                logger.info(f"  æœ€å¾Œå˜—è©¦ä½¿ç”¨ OCR...")
                try:
                    ocr_text, ocr_pages = SimplePDFParser.extract_text_with_ocr(pdf_path)
                    if ocr_text:
                        logger.info(f"  âœ… OCR æ•‘æ´æˆåŠŸ: {len(ocr_text)} å­—å…ƒ")
                        return ocr_text, ocr_pages
                except Exception as ocr_error:
                    logger.error(f"  âŒ OCR æ•‘æ´å¤±æ•—: {ocr_error}")
            
            return "", 0
    
    @staticmethod
    def extract_text_with_ocr(pdf_path: Path) -> tuple[str, int]:
        """ä½¿ç”¨ OCR æå– PDF æ–‡å­—"""
        if not OCR_AVAILABLE:
            logger.error("OCR å¥—ä»¶æœªå®‰è£")
            return "", 0
        
        try:
            logger.info(f"  é–‹å§‹ OCR è™•ç† (DPI: {OCR_DPI}, èªè¨€: {OCR_LANG})...")
            
            # å°‡ PDF è½‰æ›ç‚ºåœ–ç‰‡
            images = convert_from_path(
                pdf_path,
                dpi=OCR_DPI,
                fmt='png',
                thread_count=2
            )
            
            logger.info(f"  å·²è½‰æ›ç‚º {len(images)} å¼µåœ–ç‰‡")
            
            # å°æ¯å¼µåœ–ç‰‡é€²è¡Œ OCR
            text_parts = []
            for i, image in enumerate(images):
                logger.info(f"  è™•ç†ç¬¬ {i+1}/{len(images)} é ...")
                
                # ä½¿ç”¨ pytesseract é€²è¡Œ OCR
                page_text = pytesseract.image_to_string(
                    image,
                    lang=OCR_LANG,
                    config='--psm 6'  # å‡è¨­å–®ä¸€æ–‡å­—å¡Š
                )
                
                if page_text and page_text.strip():
                    text_parts.append(f"[ç¬¬ {i + 1} é ]\n{page_text}")
                    logger.debug(f"    æå–äº† {len(page_text)} å­—å…ƒ")
            
            combined_text = '\n'.join(text_parts)
            return combined_text, len(images)
            
        except Exception as e:
            logger.error(f"OCR è™•ç†å¤±æ•—: {e}")
            return "", 0
    
    @staticmethod
    def detect_doc_type(text: str, filename: str) -> str:
        """ åµæ¸¬æ–‡æª”é¡å‹ """
        # ç§»é™¤ç©ºæ ¼ï¼Œçµ±ä¸€è™•ç†ç°¡ç¹é«”
        text_normalized = text.replace(' ', '').replace('ã€€', '')
        text_lower = text.lower()
        filename_lower = filename.lower()
        
        # FMEA åˆ†æè¡¨ - å„ªå…ˆæª¢æŸ¥ï¼ˆæœ€å…·ç‰¹å¾µæ€§ï¼‰
        fmea_keywords = [
            'FEMA', 'DFMEA', 'dfmea', 'PFMEA', 'pfmea',
            'DFMEAè¡¨', 'fmeaè¡¨', 'FMEA Table',
            'å¤±æ•ˆæ¨¡å¼', 'å¤±æ•ˆåˆ†æ', 'å¤±æ•ˆæ¨¡å¼èˆ‡å½±éŸ¿åˆ†æ',
            'å¤±æ•ˆæˆå› åˆ†æ', 'æ•ˆæ‡‰åˆ†æ',
            'é¢¨éšªå„ªå…ˆæ•¸', 'RPN', 'rpn',
            'åš´é‡åº¦', 'ç™¼ç”Ÿåº¦', 'é›£æª¢åº¦',
            'åš´é‡åº¦S', 'ç™¼ç”Ÿåº¦O', 'é›£æª¢åº¦D',
            'é–‹ç™¼æ¡ˆè™ŸR/RD', 'æ¡ˆä»¶åç¨±',
        ]
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å¤šå€‹ FMEA ç‰¹å¾µ
        fmea_score = sum(1 for kw in fmea_keywords if kw.replace(' ', '') in text_normalized)
        
        if fmea_score >= 3:  # è‡³å°‘åŒ…å« 3 å€‹ FMEA ç‰¹å¾µ
            logger.debug(f"  åµæ¸¬åˆ° FMEA åˆ†æè¡¨é—œéµå­— (åŒ¹é…åº¦: {fmea_score})")
            return 'FMEA'
        
        # è¨­è®Šé€šçŸ¥å–® - å¤šç¨®é—œéµå­—çµ„åˆ
        ecn_notice_keywords = [
            'è¨­è®Šé€šçŸ¥å–®', 'è®¾å˜é€šçŸ¥å•',  # ç¹ç°¡é«”
            'è¨­è®Šç”³è«‹å–®è™Ÿ', 'è®¾å˜ç”³è¯·å•å·',
            'è¨­è®Šç”³è«‹äºº', 'è¨­è®Šèªªæ˜'
        ]
        if any(kw.replace(' ', '') in text_normalized for kw in ecn_notice_keywords):
            logger.debug(f"  åµæ¸¬åˆ°è¨­è®Šé€šçŸ¥å–®é—œéµå­—")
            return 'ECN_NOTICE'
        
        # è¨­è®Šç”³è«‹å–® - æ’é™¤ FMEAï¼ˆå› ç‚º FMEA ä¹Ÿå¯èƒ½æåˆ°è¨­è®Šï¼‰
        ecn_application_keywords = [
            'å–®è™Ÿï¼šEC-'
            'ç”³è«‹å–®ä½', 'ç·£ç”±', 'è¨­è®ŠåŸ·è¡Œ', 'ç ”ç™¼å–®ä½',
        ]
        if any(kw.replace(' ', '') in text_normalized for kw in ecn_application_keywords):
            # ç¢ºèªä¸æ˜¯ FMEA
            if fmea_score < 2:
                logger.debug(f"  åµæ¸¬åˆ°è¨­è®Šç”³è«‹å–®é—œéµå­—")
                return 'ECN_APPLICATION'
        
        # å®¢è¨´ - å¤šç¨®é—œéµå­—çµ„åˆ
        complaint_keywords = [
            'é¡§å®¢æŠ±æ€¨è™•ç†è³‡æ–™', 'ç•°å¸¸å–®è™Ÿ', 'ä¾†æºå–®è™Ÿ',
            'é–‹å–®é¡åˆ¥', 'å®¢æˆ¶ä»£è™Ÿ/åç¨±', 'å®¢æˆ¶æŠ±æ€¨', 'ä¸è‰¯æ•¸/æ‰¹',
            'æŠ±æ€¨å…§å®¹æè¿°', 'æŠ±æ€¨å†…å®¹åˆ†æ', 'ä¸è‰¯ç‡'
        ]
        if any(kw.replace(' ', '') in text_normalized for kw in complaint_keywords):
            # ç¢ºèªä¸æ˜¯å®¢è¨´æ¡ˆçš„ FMEA
            if fmea_score < 2:
                logger.debug(f"  åµæ¸¬åˆ°å®¢è¨´é—œéµå­—")
                return 'COMPLAINT'
        
        # å¾æª”ååˆ¤æ–·ï¼ˆå‚™ç”¨ï¼‰
        if any(kw in filename_lower for kw in ['fmea', 'dfmea', 'pfmea', 'FMEA']):
            logger.debug(f"  æ ¹æ“šæª”ååˆ¤æ–·ç‚º FMEA")
            return 'FMEA'
        
        if any(kw in filename_lower for kw in ['ecn', 'ç”³è«‹å–®', 'ç”³è«‹', 'engineering change']):
            logger.debug(f"  æ ¹æ“šæª”ååˆ¤æ–·ç‚ºè¨­è®Šç”³è«‹å–®")
            return 'ECN_APPLICATION'
        
        if any(kw in filename_lower for kw in ['complaint', 'å®¢è¨´', 'å®¢è¯‰', 'cpr']):
            logger.debug(f"  æ ¹æ“šæª”ååˆ¤æ–·ç‚ºå®¢è¨´")
            return 'COMPLAINT'
        
        if any(kw in filename_lower for kw in ['notice', 'é€šçŸ¥', 'é€šçŸ¥å–®']):
            logger.debug(f"  æ ¹æ“šæª”ååˆ¤æ–·ç‚ºè¨­è®Šé€šçŸ¥å–®")
            return 'ECN_NOTICE'
        
        logger.warning(f"  ç„¡æ³•è­˜åˆ¥æ–‡æª”é¡å‹ï¼Œæ¨™è¨˜ç‚º OTHER")
        return 'OTHER'

# ========== PDF è™•ç†å™¨ ==========
class PDFProcessor:
    """PDF è™•ç†å™¨"""
    
    def __init__(self):
        self.db = DatabaseManager()
        self.parser = SimplePDFParser()
        self.state = self.load_state()
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        for directory in [PDF_WATCH_DIR, PDF_PROCESSING_DIR, PDF_DONE_DIR, PDF_ERROR_DIR]:
            directory.mkdir(parents=True, exist_ok=True)
    
    def load_state(self) -> Dict:
        """è¼‰å…¥è™•ç†ç‹€æ…‹"""
        state_file = Path('/state/processor_state.json')
        if state_file.exists():
            try:
                with open(state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {}
    
    def save_state(self):
        """å„²å­˜è™•ç†ç‹€æ…‹"""
        state_file = Path('/state/processor_state.json')
        state_file.parent.mkdir(parents=True, exist_ok=True)
        with open(state_file, 'w', encoding='utf-8') as f:
            json.dump(self.state, f, indent=2, ensure_ascii=False)
    
    def calculate_file_hash(self, file_path: Path) -> str:
        """è¨ˆç®—æª”æ¡ˆ MD5 hash"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def process_pdf(self, pdf_path: Path) -> bool:
        """è™•ç†å–®å€‹ PDF æª”æ¡ˆ"""
        start_time = time.time()
        
        try:
            file_size = pdf_path.stat().st_size
            file_hash = self.calculate_file_hash(pdf_path)
            
            # æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
            if self.db.check_document_exists(file_hash):
                logger.info(f"â­ï¸  è·³éå·²è™•ç†: {pdf_path.name}")
                self.db.log_processing(pdf_path.name, file_hash, "skipped")
                return True
            
            logger.info(f"ğŸ“„ é–‹å§‹è™•ç†: {pdf_path.name} ({file_size/1024:.1f} KB)")
            
            # è¨˜éŒ„é–‹å§‹è™•ç†
            self.db.log_processing(pdf_path.name, file_hash, "processing")
            
            # ç§»åˆ°è™•ç†ä¸­ç›®éŒ„
            processing_path = PDF_PROCESSING_DIR / pdf_path.name
            pdf_path.rename(processing_path)
            
            # æå–æ–‡å­—
            text, page_count = self.parser.extract_text(processing_path)
            
            if not text:
                raise ValueError("ç„¡æ³•æå–æ–‡å­—å…§å®¹")
            
            logger.info(f"  æå–æ–‡å­—: {len(text)} å­—å…ƒ, {page_count} é ")
            
            # åµæ¸¬æ–‡æª”é¡å‹
            doc_type = self.parser.detect_doc_type(text, processing_path.stem)
            
            # å»ºç«‹æ–‡æª”ç‰©ä»¶
            doc = TechnicalDocument(
                doc_id=file_hash,
                doc_type=doc_type,
                file_name=processing_path.name,
                file_size=file_size,
                page_count=page_count,
                content=text,
                created_at=datetime.now()
            )
            
            # å­˜å…¥è³‡æ–™åº«
            if self.db.save_document(doc):
                # æˆåŠŸï¼šç§»åˆ°å®Œæˆç›®éŒ„
                done_path = PDF_DONE_DIR / processing_path.name
                processing_path.rename(done_path)
                
                process_time = int((time.time() - start_time) * 1000)
                self.db.log_processing(
                    pdf_path.name,
                    file_hash,
                    "success",
                    process_time_ms=process_time
                )
                
                # æ›´æ–°ç‹€æ…‹
                self.state[pdf_path.name] = {
                    "hash": file_hash,
                    "processed_at": datetime.now().isoformat(),
                    "doc_id": doc.doc_id,
                    "status": "success",
                    "page_count": page_count,
                    "doc_type": doc_type
                }
                self.save_state()
                
                logger.info(f"  âœ… æˆåŠŸè™•ç†ï¼Œè€—æ™‚: {process_time}ms")
                return True
            else:
                raise Exception("è³‡æ–™åº«å„²å­˜å¤±æ•—")
                
        except Exception as e:
            logger.error(f"  âŒ è™•ç†å¤±æ•—: {e}")
            
            # ç§»åˆ°éŒ¯èª¤ç›®éŒ„
            try:
                if processing_path.exists():
                    error_path = PDF_ERROR_DIR / processing_path.name
                    processing_path.rename(error_path)
            except:
                pass
            
            process_time = int((time.time() - start_time) * 1000)
            self.db.log_processing(
                pdf_path.name,
                file_hash,
                "error",
                error_message=str(e),
                process_time_ms=process_time
            )
            
            return False
    
    def scan_and_process(self):
        """æƒæä¸¦è™•ç† PDF æª”æ¡ˆ"""
        pdf_files = sorted(PDF_WATCH_DIR.glob('*.pdf'))[:BATCH_SIZE]
        
        if not pdf_files:
            logger.debug("æ²’æœ‰å¾…è™•ç†çš„ PDF æª”æ¡ˆ")
            return
        
        logger.info(f"ğŸ” ç™¼ç¾ {len(pdf_files)} å€‹ PDF æª”æ¡ˆ")
        
        for pdf_file in pdf_files:
            self.process_pdf(pdf_file)
    
    def run(self):
        """ä¸»åŸ·è¡Œè¿´åœˆ"""
        logger.info("=" * 60)
        logger.info("PDF è™•ç†å™¨æœå‹™å•Ÿå‹•")
        logger.info(f"ç›£æ§ç›®éŒ„: {PDF_WATCH_DIR}")
        logger.info(f"æƒæé–“éš”: {SCAN_INTERVAL} ç§’")
        logger.info(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
        logger.info("=" * 60)
        
        try:
            while True:
                self.scan_and_process()
                time.sleep(SCAN_INTERVAL)
        except KeyboardInterrupt:
            logger.info("\næ”¶åˆ°ä¸­æ–·è¨Šè™Ÿï¼Œæ­£åœ¨é—œé–‰...")
        finally:
            self.db.close()
            logger.info("æœå‹™å·²åœæ­¢")

# ========== ä¸»ç¨‹å¼ ==========
if __name__ == "__main__":
    processor = PDFProcessor()
    processor.run()
