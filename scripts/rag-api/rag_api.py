#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡ä»¶ç®¡ç† RAG API æœå‹™ - å¤šç´¢å¼•ç‰ˆæœ¬
"""

import os, json, logging, requests, pymysql, re
from datetime import datetime
from typing import List, Dict, Any, Optional
from requests.auth import HTTPBasicAuth
from pymysql.cursors import DictCursor
from urllib.parse import quote

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

# ==================== ç’°å¢ƒé…ç½® ====================
ES_URL = os.getenv("ES_URL", "http://elasticsearch:9200")
ES_USER = os.getenv("ES_USER", "elastic")
ES_PASS = os.getenv("ES_PASS", "admin@12345")
ES_INDEX_PATTERN = (
    "erp-ecn-notices,erp-ecn-applications,erp-complaint-records,erp-fmea,erp-structure"
)

MYSQL_HOST = os.getenv("MYSQL_HOST", "mysql")
MYSQL_PORT = int(os.getenv("MYSQL_PORT", 3306))
MYSQL_USER = os.getenv("MYSQL_USER", "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "root")
MYSQL_DATABASE = os.getenv("MYSQL_DATABASE", "fuhsin_erp_demo")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
GPT_MODEL = os.getenv("GPT_MODEL", "gpt-4o-mini")

FILE_SERVICE_PUBLIC_URL = os.getenv("FILE_SERVICE_PUBLIC_URL", "http://localhost:8088")

# ==================== æ—¥èªŒé…ç½® ====================
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# ==================== FastAPI åˆå§‹åŒ– ====================
app = FastAPI(
    title="æ–‡ä»¶ç®¡ç† RAG API",
    description="æ”¯æŒè·¨å¤šå€‹ç´¢å¼•çš„æŠ€è¡“æ–‡ä»¶æ™ºæ…§æª¢ç´¢æœå‹™",
    version="3.2.0",
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== å·¥å…·å‡½æ•¸ ====================
def clean_content(text: str, preserve_line_breaks: bool = True) -> str:
    """æ¸…ç†æ–‡æœ¬ä¸­çš„ç„¡ç”¨æ¨™è¨˜"""
    if not text:
        return text

    # ç§»é™¤é ç¢¼æ¨™è¨˜
    text = re.sub(r"\[ç¬¬\s*\d+\s*é \]", "", text)
    text = re.sub(r"ã€ç¬¬\s*\d+\s*é ã€‘", "", text)
    text = re.sub(r"Page\s+\d+", "", text, flags=re.IGNORECASE)

    if preserve_line_breaks:
        # ä¿ç•™æ›è¡Œï¼Œåªæ¸…ç†å¤šé¤˜ç©ºæ ¼
        text = re.sub(r"[ \t]+", " ", text)
        text = re.sub(r"\n{3,}", "\n\n", text)
        text = "\n".join(line.strip() for line in text.split("\n"))
    else:
        # åˆä½µæ‰€æœ‰ç©ºç™½å­—ç¬¦
        text = re.sub(r"\s+", " ", text)

    return text.strip()


# ==================== æ•¸æ“šæ¨¡å‹ ====================
class SearchRequest(BaseModel):
    query: str = Field(..., description="æœå°‹æŸ¥è©¢å­—ä¸²")
    mode: str = Field("hybrid", description="æœå°‹æ¨¡å¼: keyword | vector | hybrid")
    top_k: int = Field(10, ge=1, le=50, description="è¿”å›çµæœæ•¸é‡")
    use_gpt: bool = Field(True, description="æ˜¯å¦ä½¿ç”¨ GPT ç”Ÿæˆå›æ‡‰")
    doc_type_filter: Optional[List[str]] = Field(None, description="æ–‡ä»¶é¡å‹éæ¿¾")
    date_from: Optional[str] = Field(None, description="èµ·å§‹æ—¥æœŸ (YYYY-MM-DD)")
    date_to: Optional[str] = Field(None, description="çµæŸæ—¥æœŸ (YYYY-MM-DD)")
    department: Optional[str] = Field(None, description="éƒ¨é–€éæ¿¾")


class DocumentInfo(BaseModel):
    doc_id: str
    doc_number: str
    doc_type: Optional[str]
    title: Optional[str]
    summary: Optional[str]
    issue_date: Optional[str]
    department: Optional[str]
    applicant: Optional[str]
    product_codes: Optional[List[str]]
    keywords: Optional[List[str]]
    file_url: Optional[str]
    file_name: Optional[str]
    score: float = 0.0
    highlight: Optional[Dict] = None
    index_name: Optional[str] = None


class SearchResponse(BaseModel):
    success: bool
    query: str
    mode: str
    total: int
    documents: List[DocumentInfo]
    gpt_response: Optional[str] = None
    search_time_ms: int
    metadata: Dict[str, Any] = {}


# ==================== æ–‡ä»¶ URL è™•ç†å™¨ ====================
class FileURLHandler:
    """è™•ç†æ–‡ä»¶ URL ç”Ÿæˆ"""

    @staticmethod
    def generate_file_url(file_path: str, file_name: str = None) -> str:
        """ç”Ÿæˆæ–‡ä»¶ä¸‹è¼‰ URL"""
        if not file_path:
            return None

        if file_path.startswith("http://") or file_path.startswith("https://"):
            return file_path

        # ç§»é™¤è·¯å¾‘å‰ç¶´
        path_prefixes = ["/mnt/pdf/done/", "/mnt/pdf/done", "pdf/done/", "./pdf/done/"]
        cleaned_path = file_path
        for prefix in path_prefixes:
            if cleaned_path.startswith(prefix):
                cleaned_path = cleaned_path.replace(prefix, "", 1)
                break

        cleaned_path = cleaned_path.strip("/")
        if not cleaned_path and file_name:
            cleaned_path = file_name

        if not cleaned_path:
            logger.warning("ç„¡æ³•ç”Ÿæˆæ–‡ä»¶ URL: è·¯å¾‘ç‚ºç©º")
            return None

        encoded_path = quote(cleaned_path, safe="/")
        public_url = f"{FILE_SERVICE_PUBLIC_URL}/{encoded_path}"
        logger.debug(f"ç”Ÿæˆæ–‡ä»¶ URL: {file_path} -> {public_url}")

        return public_url


# ==================== å‘é‡ç”Ÿæˆå™¨ ====================
class VectorGenerator:
    def __init__(self):
        self.client = None
        if OPENAI_API_KEY and OpenAI:
            self.client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)
            self.model = EMBEDDING_MODEL
            logger.info(f"å‘é‡ç”Ÿæˆå™¨åˆå§‹åŒ–: {EMBEDDING_MODEL}")

    def generate(self, text: str) -> Optional[List[float]]:
        if not self.client or not text:
            return None
        try:
            response = self.client.embeddings.create(
                model=self.model, input=text[:8000]
            )
            return response.data[0].embedding
        except Exception as e:
            logger.error(f"å‘é‡ç”Ÿæˆå¤±æ•—: {e}")
            return None


# ==================== MySQL ç®¡ç†å™¨ ====================
class MySQLManager:
    def __init__(self):
        self.connection = None

    def ensure_connection(self):
        """ç¢ºä¿ MySQL é€£æ¥"""
        try:
            if not self.connection or not self.connection.open:
                self.connection = pymysql.connect(
                    host=MYSQL_HOST,
                    port=MYSQL_PORT,
                    user=MYSQL_USER,
                    password=MYSQL_PASSWORD,
                    database=MYSQL_DATABASE,
                    cursorclass=DictCursor,
                    charset="utf8mb4",
                )
                logger.info("âœ… MySQL é€£æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"MySQL é€£æ¥å¤±æ•—: {e}")
            self.connection = None

    def search_by_product_ids(self, product_ids: List[str]) -> set:
        """å¾å¤šå€‹è¡¨æœå°‹ç”¢å“ç›¸é—œæ–‡ä»¶"""
        self.ensure_connection()
        if not self.connection:
            return set()

        doc_ids = set()
        try:
            with self.connection.cursor() as cursor:
                # structured_documents
                for pid in product_ids:
                    cursor.execute(
                        """
                        SELECT original_doc_id FROM structured_documents 
                        WHERE JSON_CONTAINS(product_codes, %s)
                    """,
                        (json.dumps(pid),),
                    )
                    doc_ids.update(row["original_doc_id"] for row in cursor.fetchall())

                # å…¶ä»–è¡¨
                if product_ids:
                    placeholders = ",".join(["%s"] * len(product_ids))
                    for table in [
                        "ecn_notices",
                        "ecn_applications",
                        "complaint_records",
                    ]:
                        cursor.execute(
                            f"""
                            SELECT doc_id FROM {table} WHERE product_code IN ({placeholders})
                        """,
                            tuple(product_ids),
                        )
                        doc_ids.update(row["doc_id"] for row in cursor.fetchall())
        except Exception as e:
            logger.error(f"MySQL ç”¢å“æœå°‹å¤±æ•—: {e}")

        return doc_ids

    def search_by_keywords(self, keywords: List[str]) -> Dict[str, float]:
        """å¾å¤šå€‹è¡¨çš„é—œéµå­—æ¬„ä½æœå°‹"""
        self.ensure_connection()
        if not self.connection:
            return {}

        doc_scores = {}
        try:
            with self.connection.cursor() as cursor:
                for keyword in keywords:
                    keyword_pattern = f"%{keyword}%"

                    # æœå°‹ structured_documents çš„ summary
                    cursor.execute(
                        """
                        SELECT original_doc_id, 
                               (LENGTH(summary) - LENGTH(REPLACE(LOWER(summary), LOWER(%s), ''))) / LENGTH(%s) as score
                        FROM structured_documents 
                        WHERE summary LIKE %s
                    """,
                        (keyword, keyword, keyword_pattern),
                    )

                    for row in cursor.fetchall():
                        doc_id = row["original_doc_id"]
                        doc_scores[doc_id] = doc_scores.get(doc_id, 0) + float(
                            row["score"] or 0
                        )

                    # æœå°‹ technical_documents çš„ content
                    cursor.execute(
                        """
                        SELECT doc_id,
                               (LENGTH(content) - LENGTH(REPLACE(LOWER(content), LOWER(%s), ''))) / LENGTH(%s) * 0.5 as score
                        FROM technical_documents
                        WHERE content LIKE %s
                        LIMIT 100
                    """,
                        (keyword, keyword, keyword_pattern),
                    )

                    for row in cursor.fetchall():
                        doc_id = row["doc_id"]
                        doc_scores[doc_id] = doc_scores.get(doc_id, 0) + float(
                            row["score"] or 0
                        )

        except Exception as e:
            logger.error(f"MySQL é—œéµå­—æœå°‹å¤±æ•—: {e}")

        return doc_scores

    def get_full_content(self, doc_ids: List[str]) -> Dict[str, str]:
        """ç²å–æ–‡ä»¶çš„å®Œæ•´å…§å®¹"""
        self.ensure_connection()
        if not self.connection or not doc_ids:
            return {}

        try:
            with self.connection.cursor() as cursor:
                placeholders = ",".join(["%s"] * len(doc_ids))
                cursor.execute(
                    f"""
                    SELECT doc_id, content FROM technical_documents
                    WHERE doc_id IN ({placeholders})
                """,
                    tuple(doc_ids),
                )

                result = {
                    row["doc_id"]: row["content"] or "" for row in cursor.fetchall()
                }
                logger.info(f"âœ… ç²å– {len(result)} å€‹æ–‡ä»¶çš„å®Œæ•´å…§å®¹")
                return result
        except Exception as e:
            logger.error(f"ç²å–å®Œæ•´å…§å®¹å¤±æ•—: {e}")
            return {}

    def extract_content_snippet(
        self, content: str, keywords: List[str], max_length: int = 300
    ) -> str:
        """å¾å®Œæ•´å…§å®¹ä¸­æå–åŒ…å«é—œéµå­—çš„ç‰‡æ®µ"""
        if not content or not keywords:
            return content[:max_length] if content else ""

        # æŸ¥æ‰¾ç¬¬ä¸€å€‹é—œéµå­—å‡ºç¾çš„ä½ç½®
        min_pos = len(content)
        for keyword in keywords:
            pos = content.lower().find(keyword.lower())
            if pos != -1 and pos < min_pos:
                min_pos = pos

        # å‘å‰å¾Œæ“´å±•
        if min_pos < len(content):
            start = max(0, min_pos - 100)
            end = min(len(content), min_pos + max_length - 100)
            snippet = content[start:end]

            if start > 0:
                snippet = "..." + snippet
            if end < len(content):
                snippet = snippet + "..."

            return snippet

        return content[:max_length] + ("..." if len(content) > max_length else "")

    def extract_smart_snippets(
        self,
        content: str,
        summary: str,
        keywords: List[str],
        max_snippets: int = 3,
        snippet_length: int = 400,
    ) -> List[str]:
        """
        æ™ºèƒ½æå–å…§å®¹ç‰‡æ®µï¼Œç¢ºä¿èˆ‡æ‘˜è¦ä¸é‡è¤‡

        Args:
            content: å®Œæ•´å…§å®¹
            summary: æ‘˜è¦ï¼ˆç”¨æ–¼å»é‡ï¼‰
            keywords: é—œéµå­—åˆ—è¡¨
            max_snippets: æœ€å¤§ç‰‡æ®µæ•¸
            snippet_length: æ¯å€‹ç‰‡æ®µé•·åº¦

        Returns:
            ä¸é‡è¤‡çš„å…§å®¹ç‰‡æ®µåˆ—è¡¨
        """
        if not content or not keywords:
            return []

        # æ¸…ç†å…§å®¹
        content_clean = clean_content(content, preserve_line_breaks=True)
        summary_clean = (
            clean_content(summary, preserve_line_breaks=False) if summary else ""
        )

        snippets = []
        used_positions = set()

        # æŒ‰é—œéµå­—æŸ¥æ‰¾ç‰‡æ®µ
        for keyword in keywords[: max_snippets * 2]:
            keyword_lower = keyword.lower()
            content_lower = content_clean.lower()

            pos = 0
            while pos < len(content_lower):
                pos = content_lower.find(keyword_lower, pos)
                if pos == -1:
                    break

                # é¿å…ä½ç½®é‡ç–Š
                if any(
                    abs(pos - used_pos) < snippet_length // 2
                    for used_pos in used_positions
                ):
                    pos += 1
                    continue

                # æå–ç‰‡æ®µ
                start = max(0, pos - 100)
                end = min(len(content_clean), pos + snippet_length - 100)
                snippet = content_clean[start:end]

                # åœ¨å¥å­é‚Šç•Œèª¿æ•´
                if start > 0:
                    for i in range(min(50, len(snippet))):
                        if snippet[i] in "ã€‚ï¼ï¼Ÿ\nï¼›":
                            snippet = snippet[i + 1 :]
                            break
                    snippet = "..." + snippet

                if end < len(content_clean):
                    for i in range(len(snippet) - 1, max(0, len(snippet) - 50), -1):
                        if snippet[i] in "ã€‚ï¼ï¼Ÿ\nï¼›":
                            snippet = snippet[: i + 1]
                            break
                    snippet = snippet + "..."

                snippet = snippet.strip()

                # æª¢æŸ¥æ˜¯å¦èˆ‡ summary é‡è¤‡
                if self._is_content_similar(snippet, summary_clean):
                    pos += 1
                    continue

                # æª¢æŸ¥æ˜¯å¦èˆ‡å·²æœ‰ç‰‡æ®µé‡è¤‡
                if any(
                    self._is_content_similar(snippet, existing) for existing in snippets
                ):
                    pos += 1
                    continue

                # æª¢æŸ¥ç‰‡æ®µé•·åº¦
                if len(snippet.strip(".\n ")) < 20:
                    pos += 1
                    continue

                snippets.append(snippet)
                used_positions.add(pos)

                if len(snippets) >= max_snippets:
                    return snippets

                pos += 1

        return snippets

    def _is_content_similar(
        self, text1: str, text2: str, threshold: float = 0.7
    ) -> bool:
        """æª¢æŸ¥å…©æ®µæ–‡æœ¬æ˜¯å¦ç›¸ä¼¼"""
        if not text1 or not text2:
            return False

        # ç§»é™¤æ¨™é»å’Œç©ºç™½
        clean1 = re.sub(r"[^\w]", "", text1.lower())
        clean2 = re.sub(r"[^\w]", "", text2.lower())

        if not clean1 or not clean2:
            return False

        # åŒ…å«æª¢æŸ¥
        if clean1 in clean2 or clean2 in clean1:
            return True

        # Jaccard ç›¸ä¼¼åº¦
        set1 = set(clean1[i : i + 3] for i in range(len(clean1) - 2))
        set2 = set(clean2[i : i + 3] for i in range(len(clean2) - 2))

        if not set1 or not set2:
            return False

        intersection = len(set1 & set2)
        union = len(set1 | set2)
        similarity = intersection / union if union > 0 else 0

        return similarity > threshold


# ==================== æ–‡ä»¶æœå°‹æœå‹™ ====================
class DocumentSearchService:
    def __init__(self):
        self.es_session = requests.Session()
        if ES_USER and ES_PASS:
            self.es_session.auth = HTTPBasicAuth(ES_USER, ES_PASS)
        self.es_session.headers.update({"Content-Type": "application/json"})

        self.vector_gen = VectorGenerator()
        self.mysql = MySQLManager()
        self.file_handler = FileURLHandler()
        self.gpt_client = None

        if OPENAI_API_KEY and OpenAI:
            self.gpt_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

    def extract_product_ids(self, query: str) -> List[str]:
        """æå–ç”¢å“ç·¨è™Ÿ"""
        patterns = [
            r"[A-Z]{2,4}[\d]{2,4}[A-Z]?[\d]{0,4}[A-Z]{0,4}[\d]{0,4}[A-Z]{0,4}",
            r"\d{2,3}[-]\d{1,4}",
        ]
        product_ids = []
        for pattern in patterns:
            matches = re.findall(pattern, query.upper())
            product_ids.extend(matches)
        return list(set(product_ids))

    def extract_keywords(self, query: str) -> List[str]:
        stop_words = {"çš„", "æ˜¯", "åœ¨", "å’Œ", "å°‡", "æˆ–", "æœ‰", "ç‚º", "ç­‰", "äº†", "è«‹", "æ‰€æœ‰", "ä¾†", "å‡º", "æœªä¾†", "æ”¹å–„", "çµ±æ•´", "åˆ—å‡º"}
    
        # 1ï¸âƒ£ å…ˆæå–è‹±æ–‡æ•¸å­—çµ„åˆï¼ˆç”¢å“ä»£ç¢¼ï¼‰
        product_codes = re.findall(r'[A-Z0-9]{2,}[-]?[A-Z0-9]*', query)
        
        # 2ï¸âƒ£ æå–ä¸­æ–‡é—œéµè©ï¼šæŒ‰åœç”¨è©åˆ†å‰²
        cleaned = query
        for stop_word in stop_words:
            cleaned = cleaned.replace(stop_word, '|')
        
        # æŒ‰åˆ†éš”ç¬¦åˆ‡åˆ†ï¼Œéæ¿¾çŸ­è©
        chinese_words = [w.strip() for w in cleaned.split('|') 
                        if w.strip() and len(w.strip()) >= 2]
        
        # 3ï¸âƒ£ åˆä½µæ‰€æœ‰é—œéµå­—
        all_keywords = product_codes + chinese_words
        
        logger.info(f"ğŸ” å¾æŸ¥è©¢ '{query}' æå–åˆ°é—œéµå­—: {all_keywords}")
        
        # å¦‚æœé‚„æ˜¯æ²’æœ‰
        if not all_keywords:
            # å˜—è©¦æå–ä»»ä½• 2 å€‹å­—ä»¥ä¸Šçš„è©
            words = re.findall(r'[\u4e00-\u9fff]{2,}', query)  # åªåŒ¹é…ä¸­æ–‡ 2+ å­—
            if words:
                all_keywords = words
                logger.info(f"âš ï¸ ä½¿ç”¨ä¸­æ–‡è©çµ„: {all_keywords}")
            else:
                all_keywords = [query.strip()]
                logger.info(f"âš ï¸ ä½¿ç”¨æ•´å€‹æŸ¥è©¢: {all_keywords}")
        
        return list(set(all_keywords))[:10]

    def keyword_search(self, query: str, size: int = 10, filters: Dict = None) -> Dict:
        """å¤šç´¢å¼•é—œéµå­—æœå°‹"""
        search_body = {
            "size": size,
            "_source": {"excludes": ["original_extracted_content", "content_vector"]},
            "query": {
                "bool": {
                    "should": [
                        {
                            "multi_match": {
                                "query": query,
                                "fields": [
                                    "doc_number^10",
                                    "file_name^7",
                                    "summary^5",
                                    "keywords^7",
                                ],
                                "type": "best_fields",
                                "fuzziness": "AUTO",
                            }
                        }
                    ],
                    "minimum_should_match": 1,
                }
            },
            "highlight": {
                "fields": {
                    "summary": {"fragment_size": 150, "number_of_fragments": 2},
                    "change_description": {
                        "fragment_size": 150,
                        "number_of_fragments": 2,
                    },
                    "complaint_description": {
                        "fragment_size": 150,
                        "number_of_fragments": 2,
                    },
                },
                "pre_tags": ["<em>"],
                "post_tags": ["</em>"],
            },
        }

        try:
            response = self.es_session.post(
                f"{ES_URL}/{ES_INDEX_PATTERN}/_search", json=search_body, timeout=10
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"é—œéµå­—æœå°‹å¤±æ•—: {e}")
            return {"hits": {"hits": [], "total": {"value": 0}}}

    def vector_search(self, query: str, size: int = 10, filters: Dict = None) -> Dict:
        """å¤šç´¢å¼•å‘é‡æœå°‹"""
        query_vector = self.vector_gen.generate(query)
        if not query_vector:
            return {"hits": {"hits": [], "total": {"value": 0}}}

        search_body = {
            "size": size,
            "_source": {"excludes": ["original_extracted_content", "content_vector"]},
            "knn": {
                "field": "content_vector",
                "query_vector": query_vector,
                "k": size,
                "num_candidates": size * 10,
            },
        }

        try:
            response = self.es_session.post(
                f"{ES_URL}/{ES_INDEX_PATTERN}/_search", json=search_body, timeout=10
            )
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"å‘é‡æœå°‹å¤±æ•—: {e}")
            return {"hits": {"hits": [], "total": {"value": 0}}}

    def _merge_results(self, keyword_result: Dict, vector_result: Dict) -> Dict:
        """åˆä½µé—œéµå­—å’Œå‘é‡æœå°‹çµæœ"""
        merged_hits = []
        seen_ids = set()

        all_hits = keyword_result.get("hits", {}).get("hits", []) + vector_result.get(
            "hits", {}
        ).get("hits", [])

        for hit in all_hits:
            doc_id = hit.get("_id")
            if doc_id not in seen_ids:
                seen_ids.add(doc_id)
                merged_hits.append(hit)

        return {"hits": {"hits": merged_hits, "total": {"value": len(merged_hits)}}}

    def _process_results(
        self, es_result: Dict, mysql_scores: Dict, mysql_doc_ids: set, query: str = ""
    ) -> List[DocumentInfo]:
        """è™•ç†æœå°‹çµæœ"""
        documents = []

        # æ‰¹é‡ç²å–å®Œæ•´å…§å®¹
        doc_ids = [
            hit["_source"].get("original_doc_id")
            or hit["_source"].get("doc_id")
            or hit["_id"]
            for hit in es_result.get("hits", {}).get("hits", [])
        ]
        full_contents = self.mysql.get_full_content(doc_ids) if doc_ids else {}
        query_keywords = self.extract_keywords(query) if query else []

        for hit in es_result.get("hits", {}).get("hits", []):
            source = hit["_source"]
            doc_id = source.get("original_doc_id") or source.get("doc_id") or hit["_id"]

            # è¨ˆç®—è©•åˆ†
            es_score = hit.get("_score", 0)
            mysql_score = mysql_scores.get(doc_id, 0)
            total_score = es_score + mysql_score

            # è§£æ JSON æ¬„ä½
            product_codes = source.get("product_codes", [])
            if isinstance(product_codes, str):
                try:
                    product_codes = json.loads(product_codes)
                except:
                    product_codes = []

            keywords = source.get("keywords", [])
            if isinstance(keywords, str):
                try:
                    keywords = json.loads(keywords)
                except:
                    keywords = []

            # æ¸…ç†å…§å®¹
            summary = clean_content(
                source.get("summary", ""), preserve_line_breaks=True
            )

            highlight = hit.get("highlight", {})
            cleaned_highlight = {}
            for field, values in highlight.items():
                if isinstance(values, list):
                    cleaned_highlight[field] = [
                        clean_content(v, preserve_line_breaks=True) for v in values
                    ]
                else:
                    cleaned_highlight[field] = clean_content(
                        values, preserve_line_breaks=True
                    )

            # æ§‹å»ºé è¦½
            searchable_preview = ""
            if cleaned_highlight:
                first_highlight = next(iter(cleaned_highlight.values()), [])
                if isinstance(first_highlight, list) and first_highlight:
                    searchable_preview = first_highlight[0]

            if not searchable_preview and summary:
                searchable_preview = summary[:200] + (
                    "..." if len(summary) > 200 else ""
                )

            if searchable_preview:
                cleaned_highlight["_searchable_preview"] = [searchable_preview]

            # æå–å…§å®¹ç‰‡æ®µ
            full_content = full_contents.get(doc_id, "")
            content_snippets = []
            if full_content and query_keywords:
                # ğŸ”¥ ä½¿ç”¨æ™ºèƒ½ç‰‡æ®µæå–æ–¹æ³•
                content_snippets = self.mysql.extract_smart_snippets(
                    content=full_content,
                    summary=summary,  # å‚³å…¥æ‘˜è¦ç”¨æ–¼å»é‡
                    keywords=query_keywords,
                    max_snippets=5,  # ğŸ”¥ æœ€å¤š5å€‹ç‰‡æ®µï¼ˆåŸæœ¬æ˜¯3å€‹ï¼‰
                    snippet_length=500,  # ğŸ”¥ æ¯å€‹ç‰‡æ®µ500å­—å…ƒï¼ˆåŸæœ¬æ˜¯300ï¼‰
                )
            logger.info(content_snippets)
            if content_snippets:
                cleaned_highlight["content_snippets"] = content_snippets

            # ç”Ÿæˆæ–‡ä»¶ URL
            file_url = None
            file_name = source.get("file_name")
            file_path = source.get("file_path")

            if file_path:
                file_url = self.file_handler.generate_file_url(file_path, file_name)
            elif file_name:
                file_url = self.file_handler.generate_file_url(file_name)

            # ç”Ÿæˆæ¨™é¡Œ
            title = (
                file_name.replace(".pdf", "")
                if file_name
                else (
                    source.get("title")
                    or f"{source.get('doc_type', 'æ–‡ä»¶')} - {source.get('doc_number')}"
                    if source.get("doc_number")
                    else "æŠ€è¡“æ–‡ä»¶"
                )
            )

            doc_info = DocumentInfo(
                doc_id=doc_id,
                doc_number=source.get("doc_number")
                or source.get("notice_number")
                or source.get("application_number")
                or "",
                doc_type=source.get("doc_type"),
                title=title,
                summary=summary,
                issue_date=source.get("doc_date")
                or source.get("ecn_date")
                or source.get("complaint_date"),
                department=source.get("department"),
                applicant=source.get("applicant") or source.get("responsible_person"),
                product_codes=product_codes if product_codes else None,
                keywords=keywords if keywords else None,
                file_url=file_url,
                file_name=file_name,
                score=round(total_score, 3),
                highlight=cleaned_highlight if cleaned_highlight else None,
                index_name=hit.get("_index"),
            )

            documents.append(doc_info)

        documents.sort(key=lambda x: x.score, reverse=True)
        return documents

    def _generate_gpt_response(
        self, query: str, documents: List[DocumentInfo]
    ) -> Optional[str]:
        """ä½¿ç”¨ GPT ç”Ÿæˆæ™ºæ…§å›æ‡‰"""
        if not self.gpt_client or not documents:
            return None

        try:
            context_parts = []
            for idx, doc in enumerate(documents[:5], start=1):
                doc_identifier = doc.doc_number or doc.title or f"æ–‡ä»¶ {idx}"
                products_str = (
                    ", ".join(doc.product_codes) if doc.product_codes else "ç„¡"
                )

                context_parts.append(
                    f"""
ã€{doc_identifier}ã€‘
é¡å‹: {doc.doc_type or 'æŠ€è¡“æ–‡ä»¶'}
ç”¢å“: {products_str}
éƒ¨é–€: {doc.department or 'æœªæŒ‡å®š'}
æ‘˜è¦: {doc.summary[:200] if doc.summary else 'ç„¡æ‘˜è¦'}
                """.strip()
                )

            context = "\n\n".join(context_parts)

            messages = [
                {
                    "role": "system",
                    "content": """ä½ æ˜¯å°ˆæ¥­çš„æŠ€è¡“æ–‡ä»¶åŠ©ç†ã€‚æ ¹æ“šæœå°‹åˆ°çš„æ–‡ä»¶å…§å®¹ï¼Œæä¾›æº–ç¢ºã€æœ‰æ¢ç†çš„å›ç­”ã€‚
                    
å›ç­”æ ¼å¼ï¼š
ã€ä¸»è¦ç™¼ç¾ã€‘
æ ¹æ“šæ–‡ä»¶ XXXï¼Œä¸»è¦å…§å®¹ç‚º...

ã€ç›¸é—œç”¢å“ã€‘
æ¶‰åŠç”¢å“ç·¨è™Ÿï¼š...

ã€å»ºè­°ã€‘
å»ºè­°åƒè€ƒæ–‡ä»¶ XXX ä»¥äº†è§£æ›´å¤šç´°ç¯€ã€‚""",
                },
                {
                    "role": "user",
                    "content": f"æŸ¥è©¢: {query}\n\nç›¸é—œæ–‡ä»¶:\n{context}\n\nè«‹æ ¹æ“šä»¥ä¸Šæ–‡ä»¶å›ç­”æŸ¥è©¢ã€‚",
                },
            ]

            response = self.gpt_client.chat.completions.create(
                model=GPT_MODEL, messages=messages, max_tokens=500, temperature=0.7
            )

            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"GPT å›æ‡‰ç”Ÿæˆå¤±æ•—: {e}")
            return None

    def hybrid_search(self, request: SearchRequest) -> SearchResponse:
        """æ··åˆæœå°‹"""
        start_time = datetime.now()
        query = request.query

        # æå–ç”¢å“ç·¨è™Ÿå’Œé—œéµå­—
        product_ids = self.extract_product_ids(query)
        keywords = self.extract_keywords(query)

        logger.info(f"æœå°‹æŸ¥è©¢: {query}")
        logger.info(f"è­˜åˆ¥ç”¢å“ç·¨è™Ÿ: {product_ids}")
        logger.info(f"æå–é—œéµå­—: {keywords}")

        # MySQL è¼”åŠ©æŸ¥è©¢
        mysql_doc_ids = set()
        mysql_scores = {}

        if product_ids:
            product_doc_ids = self.mysql.search_by_product_ids(product_ids)
            mysql_doc_ids.update(product_doc_ids)
            for doc_id in product_doc_ids:
                mysql_scores[doc_id] = mysql_scores.get(doc_id, 0) + 10

        if keywords:
            keyword_scores = self.mysql.search_by_keywords(keywords)
            mysql_doc_ids.update(keyword_scores.keys())
            for doc_id, score in keyword_scores.items():
                mysql_scores[doc_id] = mysql_scores.get(doc_id, 0) + score * 2

        # Elasticsearch æœå°‹
        if request.mode == "keyword":
            es_result = self.keyword_search(query, request.top_k * 2)
        elif request.mode == "vector":
            es_result = self.vector_search(query, request.top_k * 2)
        else:
            keyword_result = self.keyword_search(query, request.top_k)
            vector_result = self.vector_search(query, request.top_k)
            es_result = self._merge_results(keyword_result, vector_result)

        # è™•ç†çµæœ
        final_documents = self._process_results(
            es_result, mysql_scores, mysql_doc_ids, query=query
        )
        final_documents = final_documents[: request.top_k]

        # ç”Ÿæˆ GPT å›æ‡‰
        gpt_response = None
        if request.use_gpt and self.gpt_client and final_documents:
            gpt_response = self._generate_gpt_response(query, final_documents)

        search_time = int((datetime.now() - start_time).total_seconds() * 1000)

        return SearchResponse(
            success=True,
            query=query,
            mode=request.mode,
            total=len(final_documents),
            documents=final_documents,
            gpt_response=gpt_response,
            search_time_ms=search_time,
            metadata={
                "mysql_hits": len(mysql_doc_ids),
                "product_ids_found": product_ids,
                "keywords_used": keywords,
                "indices_searched": ES_INDEX_PATTERN,
            },
        )


# ==================== åˆå§‹åŒ–æœå‹™ ====================
search_service = DocumentSearchService()


# ==================== API ç«¯é» ====================
@app.get("/")
async def root():
    return {
        "service": "æ–‡ä»¶ç®¡ç† RAG API",
        "version": "3.2.0",
        "indices": ES_INDEX_PATTERN,
        "file_service": FILE_SERVICE_PUBLIC_URL,
        "status": "running",
    }


@app.get("/health")
async def health_check():
    try:
        es_health = search_service.es_session.get(
            f"{ES_URL}/_cluster/health", timeout=5
        )
        es_status = es_health.status_code == 200

        search_service.mysql.ensure_connection()
        mysql_status = search_service.mysql.connection is not None

        return {
            "status": "healthy" if (es_status and mysql_status) else "degraded",
            "elasticsearch": es_status,
            "mysql": mysql_status,
            "openai": search_service.gpt_client is not None,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        logger.error(f"å¥åº·æª¢æŸ¥å¤±æ•—: {e}")
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat(),
        }


@app.get("/stats")
async def get_statistics():
    """ç²å–ç³»çµ±çµ±è¨ˆè³‡è¨Š"""
    try:
        indices = ES_INDEX_PATTERN.split(",")
        index_counts = {}
        total_docs = 0

        for index in indices:
            try:
                count_response = search_service.es_session.get(
                    f"{ES_URL}/{index}/_count", timeout=5
                )
                if count_response.status_code == 200:
                    count = count_response.json().get("count", 0)
                    index_counts[index] = count
                    total_docs += count
            except:
                index_counts[index] = 0

        return {
            "success": True,
            "stats": {"total_documents": total_docs, "index_counts": index_counts},
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        logger.error(f"ç²å–çµ±è¨ˆå¤±æ•—: {e}")
        return {"success": False, "error": str(e)}


@app.post("/query", response_model=SearchResponse)
async def search_documents(request: SearchRequest):
    """æ–‡ä»¶æœå°‹ç«¯é»"""
    try:
        logger.info(f"æ”¶åˆ°æœå°‹è«‹æ±‚: {request.query}, æ¨¡å¼: {request.mode}")
        response = search_service.hybrid_search(request)
        return response
    except Exception as e:
        logger.error(f"æœå°‹å¤±æ•—: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/document/{doc_id}")
async def get_document(doc_id: str):
    """ç²å–å–®ä¸€æ–‡ä»¶è©³æƒ…"""
    try:
        docs = search_service.mysql.get_document_details([doc_id])
        if not docs:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")

        doc = docs[0]
        file_path = doc.get("file_path") or doc.get("file_name")
        if file_path:
            doc["file_url"] = search_service.file_handler.generate_file_url(file_path)
            doc["download_url"] = doc["file_url"]

        return {
            "success": True,
            "document": doc,
            "related_documents": doc.get("related_doc_numbers", []),
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç²å–æ–‡ä»¶å¤±æ•—: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.on_event("startup")
async def startup_event():
    logger.info("=" * 50)
    logger.info("æ–‡ä»¶ç®¡ç† RAG API æœå‹™å•Ÿå‹•")
    logger.info(f"Elasticsearch: {ES_URL}")
    logger.info(f"ç´¢å¼•æ¨¡å¼: {ES_INDEX_PATTERN}")
    logger.info(f"MySQL: {MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}")
    logger.info(f"æ–‡ä»¶æœå‹™: {FILE_SERVICE_PUBLIC_URL}")
    logger.info(f"GPT Model: {GPT_MODEL if search_service.gpt_client else 'Disabled'}")
    logger.info("=" * 50)


@app.on_event("shutdown")
async def shutdown_event():
    logger.info("æ–‡ä»¶ç®¡ç† RAG API æœå‹™é—œé–‰")
    if search_service.mysql.connection:
        search_service.mysql.connection.close()


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8010, log_level="info")
