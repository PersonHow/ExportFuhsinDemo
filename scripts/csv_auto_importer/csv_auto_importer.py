#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSV è‡ªå‹•åŒ¯å…¥æœå‹™ - åµæ¸¬ CSV æª”æ¡ˆä¸¦è‡ªå‹•åŒ¯å…¥åˆ°å°æ‡‰è³‡æ–™è¡¨
"""

import os, time, json, hashlib, logging, pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, List
from sqlalchemy import create_engine
from urllib.parse import quote_plus

# ============== é…ç½® ==============
MYSQL_HOST = os.getenv("MYSQL_HOST", "mysql")
MYSQL_PORT = int(os.getenv("MYSQL_PORT", "3306"))
MYSQL_USER = os.getenv("MYSQL_USER", "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "root")
MYSQL_DATABASE = os.getenv("MYSQL_DATABASE", "fuhsin_erp_demo")

WATCH_DIR = Path(os.getenv("CSV_WATCH_DIR", "/csv/incoming"))
DONE_DIR = WATCH_DIR / ".done"
ERROR_DIR = WATCH_DIR / ".error"
STATE_FILE = Path("/state/.csv_import_state.json")
LOG_FILE = Path("/logs/csv_importer/csv_importer.log")

SCAN_INTERVAL = int(os.getenv("CSV_SCAN_INTERVAL", "10"))
CHUNK_SIZE = int(os.getenv("CSV_CHUNK_SIZE", "5000"))

# è³‡æ–™è¡¨æ˜ å°„ï¼ˆCSV æª”åå‰ç¶´ -> è³‡æ–™è¡¨åç¨±ï¼‰
# æ•¸å­—è¶Šå°å„ªå…ˆç´šè¶Šé«˜ï¼ˆä¸»è¡¨å¿…é ˆå…ˆåŒ¯å…¥ï¼‰
TABLE_MAPPING = {
    "technical_documents": {"table": "technical_documents", "priority": 1},
    "structured_documents": {"table": "structured_documents", "priority": 2},
    "ecn_notices": {"table": "ecn_notices", "priority": 3},
    "ecn_applications": {"table": "ecn_applications", "priority": 3},
    "complaint_records": {"table": "complaint_records", "priority": 3},
    "fmea_records": {"table": "fmea_records", "priority": 3},
    "pdf_processing_log": {"table": "pdf_processing_log", "priority": 3},
}

# ============== æ—¥èªŒè¨­å®š ==============
os.makedirs(LOG_FILE.parent, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============== ç‹€æ…‹ç®¡ç† ==============
class StateManager:
    def __init__(self):
        STATE_FILE.parent.mkdir(parents=True, exist_ok=True)
        self.state = self._load_state()

    def _load_state(self) -> Dict:
        if STATE_FILE.exists():
            try:
                return json.loads(STATE_FILE.read_text())
            except:
                return {}
        return {}

    def _save_state(self):
        STATE_FILE.write_text(json.dumps(self.state, indent=2, ensure_ascii=False))

    def is_processed(self, file_hash: str) -> bool:
        return file_hash in self.state

    def mark_processed(self, file_hash: str, file_name: str, table: str, rows: int):
        self.state[file_hash] = {
            "file_name": file_name,
            "table": table,
            "rows": rows,
            "processed_at": datetime.now().isoformat()
        }
        self._save_state()

# ============== CSV åŒ¯å…¥å™¨ ==============
class CSVImporter:
    def __init__(self):
        self.state_mgr = StateManager()
        for d in [DONE_DIR, ERROR_DIR]:
            d.mkdir(parents=True, exist_ok=True)
        # å»ºç«‹ SQLAlchemy engine
        password = quote_plus(MYSQL_PASSWORD)
        db_url = f"mysql+pymysql://{MYSQL_USER}:{password}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}?charset=utf8mb4"
        self.engine = create_engine(db_url, pool_pre_ping=True)

    def get_table_info(self, filename: str) -> Optional[Dict]:
        """å¾æª”ååˆ¤æ–·å°æ‡‰çš„è³‡æ–™è¡¨å’Œå„ªå…ˆç´š"""
        for prefix, info in TABLE_MAPPING.items():
            if filename.startswith(prefix):
                return info
        return None

    def import_csv(self, csv_path: Path) -> bool:
        """åŒ¯å…¥å–®å€‹ CSV æª”æ¡ˆ"""
        file_hash = hashlib.md5(csv_path.read_bytes()).hexdigest()
        
        if self.state_mgr.is_processed(file_hash):
            logger.info(f"â­ï¸  è·³éå·²è™•ç†: {csv_path.name}")
            return True

        table_info = self.get_table_info(csv_path.name)
        if not table_info:
            logger.warning(f"âš ï¸  ç„¡æ³•è­˜åˆ¥è³‡æ–™è¡¨: {csv_path.name}")
            return False

        table_name = table_info["table"]

        try:
            logger.info(f"ğŸ“¥ é–‹å§‹åŒ¯å…¥: {csv_path.name} -> {table_name}")
            
            # è®€å– CSV
            df = pd.read_csv(csv_path, encoding='utf-8')
            
            # åˆªé™¤ id æ¬„ä½ï¼ˆè®“è³‡æ–™åº« AUTO_INCREMENT è‡ªå‹•ç”Ÿæˆï¼‰
            if 'id' in df.columns:
                df = df.drop(columns=['id'])
                logger.info(f"ğŸ”§ å·²ç§»é™¤ id æ¬„ä½ï¼Œä½¿ç”¨è³‡æ–™åº«è‡ªå‹•ç”Ÿæˆ")
            
            total_rows = len(df)
            logger.info(f"ğŸ“Š å…± {total_rows} ç­†è³‡æ–™")

            # åˆ†æ‰¹åŒ¯å…¥ï¼ˆä½¿ç”¨ SQLAlchemy engineï¼‰
            imported = 0
            for start in range(0, total_rows, CHUNK_SIZE):
                chunk = df.iloc[start:start + CHUNK_SIZE]
                chunk.to_sql(
                    name=table_name,
                    con=self.engine,
                    if_exists='append',
                    index=False,
                    method='multi'
                )
                imported += len(chunk)
                logger.info(f"  âœ“ å·²åŒ¯å…¥ {imported}/{total_rows} ç­†")

            # æ¨™è¨˜å®Œæˆ
            self.state_mgr.mark_processed(file_hash, csv_path.name, table_name, total_rows)
            
            # ç§»å‹•åˆ°å®Œæˆç›®éŒ„
            done_path = DONE_DIR / f"{csv_path.stem}_{datetime.now().strftime('%Y%m%d%H%M%S')}{csv_path.suffix}"
            csv_path.rename(done_path)
            
            logger.info(f"âœ… å®Œæˆ: {csv_path.name} ({total_rows} ç­†)")
            return True

        except Exception as e:
            logger.error(f"âŒ åŒ¯å…¥å¤±æ•—: {csv_path.name} - {e}")
            # ç§»å‹•åˆ°éŒ¯èª¤ç›®éŒ„
            error_path = ERROR_DIR / f"{csv_path.stem}_ERROR_{datetime.now().strftime('%Y%m%d%H%M%S')}{csv_path.suffix}"
            csv_path.rename(error_path)
            return False

    def scan_and_import(self):
        """æƒæä¸¦æŒ‰å„ªå…ˆç´šé †åºåŒ¯å…¥æ‰€æœ‰ CSV æª”æ¡ˆ"""
        csv_files = list(WATCH_DIR.glob("*.csv"))
        
        if not csv_files:
            return

        # ä¾å„ªå…ˆç´šæ’åºï¼ˆå…ˆè™•ç†ä¸»è¡¨ï¼‰
        def get_priority(file_path):
            info = self.get_table_info(file_path.name)
            return (info["priority"] if info else 999, file_path.name)
        
        csv_files.sort(key=get_priority)
        
        logger.info(f"ğŸ” ç™¼ç¾ {len(csv_files)} å€‹ CSV æª”æ¡ˆ")
        
        for csv_file in csv_files:
            self.import_csv(csv_file)

    def run(self):
        """ä¸»å¾ªç’°"""
        logger.info(f"ğŸš€ CSV è‡ªå‹•åŒ¯å…¥æœå‹™å•Ÿå‹•")
        logger.info(f"ğŸ“‚ ç›£æ§ç›®éŒ„: {WATCH_DIR}")
        logger.info(f"ğŸ”„ æƒæé–“éš”: {SCAN_INTERVAL} ç§’")
        
        while True:
            try:
                self.scan_and_import()
                time.sleep(SCAN_INTERVAL)
            except KeyboardInterrupt:
                logger.info("â¹ï¸  æœå‹™åœæ­¢")
                break
            except Exception as e:
                logger.error(f"âŒ ä¸»å¾ªç’°éŒ¯èª¤: {e}")
                time.sleep(SCAN_INTERVAL)

# ============== ä¸»ç¨‹å¼ ==============
if __name__ == "__main__":
    importer = CSVImporter()
    importer.run()