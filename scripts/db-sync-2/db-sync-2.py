#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è³‡æ–™åº«åŒæ­¥è…³æœ¬ - å¤šè¡¨åŒæ­¥ç‰ˆ
åŒæ­¥ PDF ç›¸é—œè¡¨åˆ° Elasticsearch
"""

import os, sys, time, json, pymysql, requests, signal
from datetime import datetime, date
from decimal import Decimal
from typing import Dict, List, Any, Optional
from pymysql.cursors import DictCursor
from requests.auth import HTTPBasicAuth
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

# ========== ç’°å¢ƒè®Šæ•¸é…ç½® ==========
ES_URL = os.environ.get('ES_URL', 'http://localhost:9200')
ES_USER = os.environ.get('ES_USER', 'elastic')
ES_PASS = os.environ.get('ES_PASS', 'admin@12345')

MYSQL_HOST = os.environ.get('MYSQL_HOST', 'mysql')
MYSQL_PORT = int(os.environ.get('MYSQL_PORT', '3306'))
MYSQL_USER = os.environ.get('MYSQL_USER', 'root')
MYSQL_PASS = os.environ.get('MYSQL_PASS', 'root')
MYSQL_DB = os.environ.get('MYSQL_DB', 'fuhsin_erp_demo')

BATCH_SIZE = int(os.environ.get('DB_BATCH_SIZE', '1000'))
PAGE_SIZE = int(os.environ.get('DB_PAGE_SIZE', '5000'))
PARALLEL_THREADS = int(os.environ.get('PARALLEL_THREADS', '4'))
SYNC_INTERVAL = int(os.environ.get('DB_SYNC_INTERVAL', '60'))

# è‡ªå‹•åœæ­¢é…ç½®
AUTO_STOP_ENABLED = os.environ.get("AUTO_STOP_ENABLED", "false").lower() in ("true", "1", "yes")
AUTO_STOP_EMPTY_ROUNDS = int(os.environ.get("AUTO_STOP_EMPTY_ROUNDS", "3"))

# ç‹€æ…‹æª”é…ç½®
STATE_FILE = os.environ.get("STATE_FILE", "/state/.sync_state.json")

# ========== æ—¥èªŒé…ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

should_stop = False

def to_bool(v):
    if v is None: return None
    if isinstance(v, bool): return v
    if isinstance(v, (int, float)): return int(v) != 0
    if isinstance(v, str):
        s = v.strip().lower()
        if s in ('1','true','yes','y','on'): return True
        if s in ('0','false','no','n','off',''): return False
    return None


# ========== ç‹€æ…‹ç®¡ç† ==========
class StateManager:
    """ç®¡ç†åŒæ­¥ç‹€æ…‹çš„æŒä¹…åŒ–"""
    def __init__(self, state_file: str = STATE_FILE):
        self.state_file = state_file
        self.state = self._load_state()
    
    def _load_state(self) -> Dict:
        """è¼‰å…¥ç‹€æ…‹æª”"""
        try:
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            state_dir = os.path.dirname(self.state_file)
            if state_dir:
                os.makedirs(state_dir, exist_ok=True)
            
            if os.path.exists(self.state_file):
                with open(self.state_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"âš ï¸  ç„¡æ³•è®€å–ç‹€æ…‹æª”: {e}")
        return {}
    
    def _save_state(self):
        """ä¿å­˜ç‹€æ…‹æª”"""
        try:
            state_dir = os.path.dirname(self.state_file)
            if state_dir:
                os.makedirs(state_dir, exist_ok=True)
            
            with open(self.state_file, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False, default=str)
        except Exception as e:
            logger.error(f"âŒ ç„¡æ³•ä¿å­˜ç‹€æ…‹æª”: {e}")
    
    def get_last_sync_time(self, table_name: str) -> Optional[str]:
        """ç²å–è¡¨çš„æœ€å¾ŒåŒæ­¥æ™‚é–“"""
        return self.state.get(table_name, {}).get('last_modified')
    
    def update_sync_time(self, table_name: str, last_modified: datetime, record_count: int):
        """æ›´æ–°è¡¨çš„åŒæ­¥æ™‚é–“"""
        self.state[table_name] = {
            'last_modified': last_modified.isoformat(),
            'record_count': record_count,
            'synced_at': datetime.now().isoformat()
        }
        self._save_state()

# ========== Elasticsearch å®¢æˆ¶ç«¯ ==========
class ElasticsearchClient:
    def __init__(self):
        self.session = requests.Session()
        if ES_USER and ES_PASS:
            self.session.auth = HTTPBasicAuth(ES_USER, ES_PASS)
        self.session.headers.update({'Content-Type': 'application/json'})
        
    def check_connection(self):
        """æª¢æŸ¥ Elasticsearch é€£æ¥"""
        try:
            response = self.session.get(f"{ES_URL}/_cluster/health")
            if response.status_code == 200:
                health = response.json()
                logger.info(f"âœ… Elasticsearch é€£æ¥æˆåŠŸï¼Œç‹€æ…‹: {health['status']}")
                return True
            else:
                logger.error(f"âŒ Elasticsearch é€£æ¥å¤±æ•—: {response.status_code}")
                return False
        except Exception as e:
            logger.error(f"âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearch: {e}")
            return False
    
    def create_index(self, index_name: str, doc_type: str = 'general'):
        """å»ºç«‹ç´¢å¼•ä¸¦è¨­å®š mapping"""
        try:
            # æª¢æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            response = self.session.head(f"{ES_URL}/{index_name}")
            if response.status_code == 200:
                logger.debug(f"ç´¢å¼• {index_name} å·²å­˜åœ¨")
                return True
            
            # å»ºç«‹æ–°ç´¢å¼•
            mapping = self._get_mapping_for_type(doc_type)
            response = self.session.put(
                f"{ES_URL}/{index_name}",
                json=mapping
            )
            
            if response.status_code in [200, 201]:
                logger.info(f"âœ… æˆåŠŸå»ºç«‹ç´¢å¼•: {index_name}")
                return True
            else:
                logger.error(f"âŒ å»ºç«‹ç´¢å¼•å¤±æ•—: {response.text}")
                return False
                
        except Exception as e:
            logger.error(f"âŒ å»ºç«‹ç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    def _get_mapping_for_type(self, doc_type: str) -> dict:
        """æ ¹æ“šæ–‡æª”é¡å‹ç²å–å°æ‡‰çš„ mapping"""
        base_mapping = {
            "settings": {
                "number_of_shards": 2,
                "number_of_replicas": 1,
                "refresh_interval": "30s",
                "analysis": {
                    "analyzer": {
                        "chinese_analyzer": {
                            "type": "standard",
                            "stopwords": "_chinese_"
                        }
                    }
                }
            },
            "mappings": {
                "properties": {
                    "doc_id": {"type": "keyword"},
                    "created_at": {"type": "date"},
                    "last_modified": {"type": "date"}
                }
            }
        }
        
        # æ ¹æ“šé¡å‹æ·»åŠ ç‰¹å®šæ¬„ä½
        if doc_type == 'ecn_notice':
            base_mapping["mappings"]["properties"].update({
                "notice_number": {"type": "keyword"},
                "application_number": {"type": "keyword"},
                "product_code": {"type": "keyword"},
                "product_name": {
                    "type": "text",
                    "analyzer": "chinese_analyzer",
                    "fields": {"keyword": {"type": "keyword"}}
                },
                "change_description": {"type": "text", "analyzer": "chinese_analyzer"},
                "before_change": {"type": "text", "analyzer": "chinese_analyzer"},
                "after_change": {"type": "text", "analyzer": "chinese_analyzer"},
                "inventory_handling": {"type": "text", "analyzer": "chinese_analyzer"},
                "applicant": {"type": "keyword"},
                "ecn_date": {"type": "date"}
            })
        
        elif doc_type == 'ecn_application':
            base_mapping["mappings"]["properties"].update({
                "application_number": {"type": "keyword"},
                "product_code": {"type": "keyword"},
                "product_name": {
                    "type": "text",
                    "analyzer": "chinese_analyzer",
                    "fields": {"keyword": {"type": "keyword"}}
                },
                "reason": {"type": "text", "analyzer": "chinese_analyzer"},
                "change_items": {"type": "text", "analyzer": "chinese_analyzer"},
                "change_before": {"type": "text", "analyzer": "chinese_analyzer"},
                "change_after": {"type": "text", "analyzer": "chinese_analyzer"},
                "meeting_suggestions": {"type": "text", "analyzer": "chinese_analyzer"},
                "review_notes": {"type": "text", "analyzer": "chinese_analyzer"},
                "ecn_date": {"type": "date"}
            })
        
        elif doc_type == 'complaint':
            base_mapping["mappings"]["properties"].update({
                "complaint_number": {"type": "keyword"},
                "complaint_type": {"type": "keyword"},
                "customer_code": {"type": "keyword"},
                "customer_name": {
                    "type": "text",
                    "analyzer": "chinese_analyzer",
                    "fields": {"keyword": {"type": "keyword"}}
                },
                "product_code": {"type": "keyword"},
                "product_name": {
                    "type": "text",
                    "analyzer": "chinese_analyzer",
                    "fields": {"keyword": {"type": "keyword"}}
                },
                "complaint_description": {"type": "text", "analyzer": "chinese_analyzer"},
                "complaint_analysis": {"type": "text", "analyzer": "chinese_analyzer"},
                "responsible_sales": {"type": "keyword"}
            })
        
        elif doc_type == 'fmea':
            base_mapping["mappings"]["properties"].update({
                "case_number": {"type": "keyword"},
                "case_name": {
                    "type": "text",
                    "analyzer": "chinese_analyzer",
                    "fields": {"keyword": {"type": "keyword"}}
                },
                "analysis_type": {"type": "keyword"},
                "product_type": {"type": "keyword"},
                "responsible_person": {"type": "keyword"},
                "analyst": {"type": "text", "analyzer": "chinese_analyzer"},
                "analysis_item": {"type": "text", "analyzer": "chinese_analyzer"},
                "failure_mode": {"type": "text", "analyzer": "chinese_analyzer"},
                "failure_effect": {"type": "text", "analyzer": "chinese_analyzer"},
                "failure_cause": {"type": "text", "analyzer": "chinese_analyzer"},
                "severity_s": {"type": "integer"},
                "occurrence_o": {"type": "integer"},
                "detection_d": {"type": "integer"},
                "rpn": {"type": "integer"},
                "current_control": {"type": "text", "analyzer": "chinese_analyzer"},
                "corrective_action": {"type": "text", "analyzer": "chinese_analyzer"},
                "improvement_result": {"type": "text", "analyzer": "chinese_analyzer"},
                "is_customer_complaint": {"type": "boolean"},
                "department_head": {"type": "keyword"},
                "section_head": {"type": "keyword"},
                "form_date": {"type": "date"},
                "revision_date": {"type": "date"}
            })
        
        elif doc_type == 'document':
            base_mapping["mappings"]["properties"].update({
                "original_doc_id": {"type": "keyword"},
                "doc_type": {"type": "keyword"},
                "doc_number": {"type": "keyword"},
                "doc_date": {"type": "date"},
                "file_name": {"type": "keyword"},
                "file_url": {"type": "keyword"},
                "product_codes": {"type": "keyword"},
                "product_names": {
                    "type": "text",
                    "analyzer": "chinese_analyzer",
                    "fields": {"keyword": {"type": "keyword"}}
                },
                "applicant": {"type": "keyword"},
                "department": {"type": "keyword"},
                "summary": {"type": "text", "analyzer": "chinese_analyzer"},
                "keywords": {"type": "keyword"},
                "status": {"type": "keyword"},
                "priority": {"type": "keyword"}
            })
        
        return base_mapping
    
    def bulk_index(self, index_name: str, documents: List[Dict]) -> int:
        """ æ‰¹æ¬¡ç´¢å¼•æ–‡æª” """
        if not documents:
            return 0
        
        try:
            # å»ºç«‹ bulk è«‹æ±‚
            lines = []
            for doc in documents:
                doc_id = doc.get('id') or doc.get('doc_id')
                # ç´¢å¼•å‘½ä»¤
                lines.append(json.dumps({"index": {"_index": index_name, "_id": doc_id}}))
                # æ–‡æª”å…§å®¹
                lines.append(json.dumps(doc, ensure_ascii=False, default=str))
            
            bulk_data = '\n'.join(lines) + '\n'
            
            # ç™¼é€ bulk è«‹æ±‚
            response = self.session.post(
                f"{ES_URL}/_bulk",
                data=bulk_data,
                headers={'Content-Type': 'application/x-ndjson'}
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get('errors'):
                    # â­ ä¿®æ”¹é€™è£¡ï¼šè¼¸å‡ºè©³ç´°éŒ¯èª¤
                    error_items = [item for item in result['items'] if 'error' in item.get('index', {})]
                    for item in error_items[:5]:  # åªé¡¯ç¤ºå‰3å€‹éŒ¯èª¤
                        error_detail = item.get('index', {}).get('error', {})
                        logger.error(f"ç´¢å¼•éŒ¯èª¤è©³æƒ…: {json.dumps(error_detail, ensure_ascii=False, indent=2)}")
                    
                    error_count = len(error_items)
                    logger.warning(f"æ‰¹æ¬¡ç´¢å¼•éƒ¨åˆ†å¤±æ•—: {error_count}/{len(documents)} éŒ¯èª¤")
                    return len(documents) - error_count
                return len(documents)
            else:
                logger.error(f"æ‰¹æ¬¡ç´¢å¼•å¤±æ•—: {response.status_code} - {response.text[:500]}")
                return 0
                
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return 0
    
    def get_doc_count(self, index_name: str) -> int:
        """ç²å–ç´¢å¼•ä¸­çš„æ–‡æª”æ•¸é‡"""
        try:
            response = self.session.get(f"{ES_URL}/{index_name}/_count")
            if response.status_code == 200:
                return response.json().get('count', 0)
            return 0
        except Exception:
            return 0

# ========== MySQL åŒæ­¥å™¨ ==========
class MySQLSyncer:
    def __init__(self, es_client: ElasticsearchClient):
        self.es_client = es_client
        self.connection = None
        self.state_mgr = StateManager()
        self.last_doc_counts = {}  # è¿½è¹¤æ¯å€‹ç´¢å¼•çš„æ–‡æª”æ•¸
        
    def connect(self):
        """é€£æ¥åˆ° MySQL"""
        try:
            self.connection = pymysql.connect(
                host=MYSQL_HOST,
                port=MYSQL_PORT,
                user=MYSQL_USER,
                password=MYSQL_PASS,
                database=MYSQL_DB,
                cursorclass=DictCursor,
                charset='utf8mb4'
            )
            logger.info("âœ… MySQL é€£æ¥æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ MySQL é€£æ¥å¤±æ•—: {e}")
            return False
    
    def sync_table(self, table_name: str, index_name: str, doc_type: str = 'general') -> bool:
        """åŒæ­¥å–®å€‹è³‡æ–™è¡¨ï¼Œè¿”å›æ˜¯å¦æœ‰æ–°æ•¸æ“š"""
        if not self.connection or not self.connection.open:
            if not self.connect():
                return False
        
        try:
            # å»ºç«‹æˆ–æ›´æ–°ç´¢å¼•
            self.es_client.create_index(index_name, doc_type)
            
            # ç²å–ä¸Šæ¬¡åŒæ­¥æ™‚é–“
            last_sync_time = self.state_mgr.get_last_sync_time(table_name)
            
            # æ§‹å»ºå¢é‡æŸ¥è©¢
            count_query = f"SELECT COUNT(*) as total FROM {table_name}"
            where_clause = ""
            if last_sync_time:
                where_clause = f" WHERE last_modified > '{last_sync_time}'"
                count_query += where_clause
            
            # ç²å–æ–°å¢/æ›´æ–°çš„ç­†æ•¸
            with self.connection.cursor() as cursor:
                cursor.execute(count_query)
                total = cursor.fetchone()['total']
            
            if total == 0:
                if last_sync_time:
                    logger.debug(f"ğŸ“­ {table_name} æ²’æœ‰æ–°è³‡æ–™ï¼ˆä¸Šæ¬¡åŒæ­¥: {last_sync_time[:19]}ï¼‰")
                else:
                    logger.info(f"è³‡æ–™è¡¨ {table_name} æ²’æœ‰è³‡æ–™")
                return False
            
            logger.info(f"ğŸ“Š é–‹å§‹åŒæ­¥ {table_name}: {'å¢é‡' if last_sync_time else 'å…¨é‡'} {total} ç­†è³‡æ–™")
            
            # ä½¿ç”¨å¤šåŸ·è¡Œç·’è™•ç†
            with ThreadPoolExecutor(max_workers=PARALLEL_THREADS) as executor:
                futures = []
                
                for offset in range(0, total, PAGE_SIZE):
                    future = executor.submit(
                        self._sync_batch, 
                        table_name, 
                        index_name, 
                        offset, 
                        min(PAGE_SIZE, total - offset),
                        where_clause  # å‚³é WHERE æ¢ä»¶
                    )
                    futures.append(future)
                
                # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
                indexed_total = 0
                for future in as_completed(futures):
                    try:
                        indexed = future.result()
                        indexed_total += indexed
                    except Exception as e:
                        logger.error(f"æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")
            
            # ç²å–æœ€æ–°çš„ last_modified æ™‚é–“
            with self.connection.cursor() as cursor:
                cursor.execute(f"SELECT MAX(last_modified) as max_time FROM {table_name}")
                result = cursor.fetchone()
                max_modified_time = result['max_time'] if result else datetime.now()
            
            # æ›´æ–°ç‹€æ…‹
            self.state_mgr.update_sync_time(table_name, max_modified_time, indexed_total)
            
            # ç²å–æœ€çµ‚æ–‡æª”æ•¸
            final_count = self.es_client.get_doc_count(index_name)
            logger.info(f"âœ… {table_name} åŒæ­¥å®Œæˆ: ç´¢å¼• {indexed_total} ç­†ï¼Œç¸½è¨ˆ {final_count} ç­†æ–‡æª”")
            
            return True  # æœ‰æ–°è³‡æ–™å°±è¿”å› True
            
        except Exception as e:
            logger.error(f"âŒ åŒæ­¥ {table_name} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    def _sync_batch(self, table_name: str, index_name: str, offset: int, limit: int, where_clause: str = "") -> int:
        """åŒæ­¥ä¸€æ‰¹è³‡æ–™"""
        conn = None
        try:
            # ç‚ºæ¯å€‹åŸ·è¡Œç·’å»ºç«‹ç¨ç«‹é€£æ¥
            conn = pymysql.connect(
                host=MYSQL_HOST,
                port=MYSQL_PORT,
                user=MYSQL_USER,
                password=MYSQL_PASS,
                database=MYSQL_DB,
                cursorclass=DictCursor,
                charset='utf8mb4'
            )
            
            indexed = 0
            with conn.cursor() as cursor:
                # æŸ¥è©¢è³‡æ–™ï¼ˆæ”¯æŒå¢é‡æŸ¥è©¢ï¼‰
                query = f"SELECT * FROM {table_name}{where_clause} LIMIT %s OFFSET %s"
                cursor.execute(query, (limit, offset))
                
                # æ‰¹æ¬¡è™•ç†
                batch = []
                for row in cursor:
                    # è™•ç†æ—¥æœŸæ™‚é–“æ¬„ä½
                    for key, value in row.items():
                        if isinstance(value, (datetime, date)):
                            row[key] = value.isoformat()
                        elif isinstance(value, Decimal):
                            row[key] = float(value)
                        elif isinstance(value, (bytes, bytearray, memoryview)):
                            row[key] = bytes(value).decode("utf-8", errors="ignore")
                    
                    # è™•ç† JSON æ¬„ä½ (structured_documents)
                    if table_name == 'structured_documents':
                        json_fields = ['product_codes', 'product_names', 'related_doc_numbers', 
                                        'responsible_units', 'keywords']
                        for field in json_fields:
                            if field in row and row[field]:
                                try:
                                    if isinstance(row[field], str):
                                        row[field] = json.loads(row[field])
                                except Exception:
                                    row[field] = []
                    
                    if table_name == 'fmea_records':
                        if 'is_customer_complaint' in row:
                            row['is_customer_complaint'] = to_bool(row['is_customer_complaint'])

                    batch.append(row)
                    
                    if len(batch) >= BATCH_SIZE:
                        indexed += self.es_client.bulk_index(index_name, batch)
                        batch = []
                
                # è™•ç†å‰©é¤˜çš„è³‡æ–™
                if batch:
                    indexed += self.es_client.bulk_index(index_name, batch)
            
            return indexed
            
        except Exception as e:
            logger.error(f"æ‰¹æ¬¡åŒæ­¥å¤±æ•— (offset={offset}): {e}")
            return 0
        finally:
            if conn:
                conn.close()
    
    def sync_all(self) -> bool:
        """åŒæ­¥æ‰€æœ‰é…ç½®çš„è³‡æ–™è¡¨ï¼Œè¿”å›æ˜¯å¦æœ‰ä»»ä½•æ–°æ•¸æ“š"""
        # æ–¹æ¡ˆAï¼šæ¯ç¨®è¡¨å–®åŒæ­¥åˆ°ä¸åŒç´¢å¼•
        tables = [
            # PDF æ–‡ä»¶ç›¸é—œè¡¨
            ('ecn_notices', 'erp-ecn-notices', 'ecn_notice'),
            ('ecn_applications', 'erp-ecn-applications', 'ecn_application'),
            ('complaint_records', 'erp-complaint-records', 'complaint'),
            ('fmea_records', 'erp-fmea', 'fmea'),
            ('structured_documents', 'erp-structure', 'document'),
        ]
        
        had_any_new_data = False
        for table_name, index_name, doc_type in tables:
            if should_stop:
                break
            had_new_data = self.sync_table(table_name, index_name, doc_type)
            if had_new_data:
                had_any_new_data = True
        
        return had_any_new_data
    
    def close(self):
        """é—œé–‰é€£æ¥"""
        if self.connection:
            self.connection.close()
            logger.info("MySQL é€£æ¥å·²é—œé–‰")

# ========== ä¿¡è™Ÿè™•ç† ==========
def signal_handler(signum, frame):
    global should_stop
    logger.info("\nâš ï¸ æ”¶åˆ°åœæ­¢ä¿¡è™Ÿï¼Œæ­£åœ¨å„ªé›…é—œé–‰...")
    should_stop = True

# ========== ä¸»ç¨‹å¼ ==========
def main():
    global should_stop
    
    # è¨»å†Šä¿¡è™Ÿè™•ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    # é¡¯ç¤ºé…ç½®è³‡è¨Š
    logger.info("=" * 60)
    logger.info("ğŸ“‹ è³‡æ–™åº«åŒæ­¥æœå‹™å•Ÿå‹•")
    logger.info(f"ES URL: {ES_URL}")
    logger.info(f"MySQL: {MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}")
    logger.info(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    logger.info(f"é é¢å¤§å°: {PAGE_SIZE}")
    logger.info(f"ä¸¦è¡ŒåŸ·è¡Œç·’: {PARALLEL_THREADS}")
    logger.info(f"åŒæ­¥é–“éš”: {SYNC_INTERVAL} ç§’")
    logger.info(f"ğŸ¤– è‡ªå‹•åœæ­¢ï¼š{'å•Ÿç”¨' if AUTO_STOP_ENABLED else 'åœç”¨'}")
    if AUTO_STOP_ENABLED:
        logger.info(f"   é€£çºŒç©ºè¼ªä¸Šé™ï¼š{AUTO_STOP_EMPTY_ROUNDS} æ¬¡")
    logger.info("åŒæ­¥è³‡æ–™è¡¨:")
    logger.info("  - ecn_notices â†’ erp-ecn-notices")
    logger.info("  - ecn_applications â†’ erp-ecn-applications")
    logger.info("  - complaint_records â†’ erp-complaint-records")
    logger.info("  - fmea_records â†’ erp-fmea")
    logger.info("  - structured_documents â†’ erp-documents")
    logger.info("=" * 60)
    
    # å»ºç«‹å®¢æˆ¶ç«¯
    es_client = ElasticsearchClient()
    
    # æª¢æŸ¥ Elasticsearch é€£æ¥
    while not should_stop:
        if es_client.check_connection():
            break
        logger.info("ç­‰å¾… Elasticsearch å•Ÿå‹•...")
        time.sleep(5)
    
    if should_stop:
        return
    
    # å»ºç«‹åŒæ­¥å™¨
    syncer = MySQLSyncer(es_client)
    
    try:
        # é¦–æ¬¡å…¨é‡åŒæ­¥
        logger.info("ğŸš€ é–‹å§‹é¦–æ¬¡å…¨é‡åŒæ­¥...")
        had_new_data = syncer.sync_all()
        
        # è‡ªå‹•åœæ­¢è¨ˆæ•¸å™¨
        empty_rounds = 0 if had_new_data else 1
        total_syncs = 1
        
        # å®šæœŸå¢é‡åŒæ­¥
        while not should_stop:
            # é¡¯ç¤ºç•¶å‰ç‹€æ…‹
            if not had_new_data:
                logger.info(f"ğŸ˜´ æ‰€æœ‰è³‡æ–™è¡¨éƒ½å·²åŒæ­¥å®Œæˆ (ç©ºè¼ª {empty_rounds}/{AUTO_STOP_EMPTY_ROUNDS if AUTO_STOP_ENABLED else 'âˆ'})")
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦è‡ªå‹•åœæ­¢
                if AUTO_STOP_ENABLED and empty_rounds >= AUTO_STOP_EMPTY_ROUNDS:
                    logger.info("=" * 60)
                    logger.info(f"âœ… å®Œæˆï¼æ‰€æœ‰è³‡æ–™è¡¨éƒ½å·²åŒæ­¥")
                    logger.info(f"ğŸ“Š å…±åŸ·è¡Œ {total_syncs} æ¬¡åŒæ­¥")
                    logger.info(f"ğŸ›‘ å·²é€£çºŒ {empty_rounds} è¼ªç„¡æ–°è³‡æ–™ï¼Œè‡ªå‹•åœæ­¢æœå‹™")
                    logger.info("=" * 60)
                    break
            
            logger.info(f"â° ç­‰å¾… {SYNC_INTERVAL} ç§’å¾Œé€²è¡Œä¸‹æ¬¡åŒæ­¥...")
            
            # å¯ä¸­æ–·çš„ç­‰å¾…
            for _ in range(SYNC_INTERVAL):
                if should_stop:
                    break
                time.sleep(1)
            
            if not should_stop:
                logger.info("ğŸ”„ é–‹å§‹å¢é‡åŒæ­¥...")
                had_new_data = syncer.sync_all()
                total_syncs += 1
                
                # æ›´æ–°ç©ºè¼ªè¨ˆæ•¸
                if had_new_data:
                    empty_rounds = 0  # é‡ç½®è¨ˆæ•¸å™¨
                else:
                    empty_rounds += 1
                
    except Exception as e:
        logger.error(f"âŒ ä¸»ç¨‹å¼ç™¼ç”ŸéŒ¯èª¤: {e}")
    finally:
        syncer.close()
        logger.info("ğŸ‘‹ è³‡æ–™åº«åŒæ­¥æœå‹™å·²åœæ­¢")

if __name__ == '__main__':
    main()