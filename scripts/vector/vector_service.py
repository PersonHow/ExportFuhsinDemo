#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡ç”Ÿæˆæœå‹™ - å„ªåŒ–æ–‡æœ¬æå–ç‰ˆ
æ ¹æ“šæ–‡æª”é¡å‹æå–æœ€ç›¸é—œçš„æ–‡æœ¬ç”Ÿæˆå‘é‡
"""

import os, time, json
import signal, requests, math
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from requests.auth import HTTPBasicAuth
import logging

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# ========== ç’°å¢ƒè®Šæ•¸ ==========
ES_URL = os.environ.get("ES_URL", "http://localhost:9200")
ES_USER = os.environ.get("ES_USER", "elastic")
ES_PASS = os.environ.get("ES_PASS", "admin@12345")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.environ.get("OPENAI_BASE_URL", "https://api.openai.com/v1").rstrip("/")
EMBEDDING_MODEL = os.environ.get("EMBEDDING_MODEL", "text-embedding-3-small")
INDEX_PATTERN = os.environ.get("INDEX_PATTERN", "erp-*")
BATCH_SIZE = int(os.environ.get("VECTOR_BATCH_SIZE", "100"))
SLEEP_SEC = int(os.environ.get("SLEEP", "10"))
ES_WAIT_TIMEOUT = int(os.environ.get("ES_WAIT_TIMEOUT", "180"))
REQUESTS_TIMEOUT = int(os.environ.get("REQUESTS_TIMEOUT", "30"))
MAX_RETRIES = int(os.environ.get("MAX_RETRIES", "5"))

# è‡ªå‹•åœæ­¢é…ç½®
AUTO_STOP_ENABLED = os.environ.get("AUTO_STOP_ENABLED", "false").lower() in ("true", "1", "yes")
AUTO_STOP_EMPTY_ROUNDS = int(os.environ.get("AUTO_STOP_EMPTY_ROUNDS", "3"))
AUTO_STOP_FAIL_LIMIT = int(os.environ.get("AUTO_STOP_FAIL_LIMIT", "5"))

# ========== æ—¥èªŒé…ç½® ==========
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ========== é€£ç·šç‰©ä»¶ ==========
session = requests.Session()
if ES_USER and ES_PASS:
    session.auth = HTTPBasicAuth(ES_USER, ES_PASS)

client: Optional[OpenAI] = None
if OPENAI_API_KEY and OpenAI is not None:
    client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)

_SHOULD_STOP = False

# ========== å·¥å…·æ–¹æ³• ==========
def log(msg: str) -> None:
    logger.info(msg)

def wait_for_es(timeout_sec: int = ES_WAIT_TIMEOUT) -> None:
    """ç­‰å¾… Elasticsearch è‡³å°‘é”åˆ° yellow å¥åº·ç‹€æ…‹"""
    deadline = time.time() + timeout_sec
    last_err: Optional[Exception] = None
    while time.time() < deadline:
        try:
            r = session.get(
                f"{ES_URL}/_cluster/health",
                params={"wait_for_status": "yellow", "timeout": "30s"},
                timeout=REQUESTS_TIMEOUT,
            )
            if r.ok:
                status = r.json().get("status")
                if status in ("yellow", "green"):
                    log(f"ES å°±ç·’ï¼ˆstatus={status}ï¼‰")
                    return
                log(f"ES ç‹€æ…‹ {status}ï¼Œç¹¼çºŒç­‰å¾…â€¦")
        except Exception as e:
            last_err = e
        time.sleep(3)
    raise RuntimeError(f"Elasticsearch åœ¨ {timeout_sec}s å…§æœªå°±ç·’: {last_err}")

def _sleep_backoff(i: int, base: float = 1.0) -> None:
    time.sleep(base * (2**i))

def http_get(url: str, *, params: Optional[Dict[str, Any]] = None, 
             headers: Optional[Dict[str, str]] = None, retries: int = MAX_RETRIES) -> requests.Response:
    for i in range(retries):
        try:
            r = session.get(url, params=params, headers=headers, timeout=REQUESTS_TIMEOUT)
            if r.status_code in (502, 503, 504):
                raise requests.ConnectionError(f"Transient {r.status_code}")
            return r
        except (requests.ConnectionError, requests.Timeout) as e:
            if i == retries - 1:
                raise
            log(f"GET é‡è©¦ {i+1}/{retries-1}: {e}")
            _sleep_backoff(i)
    raise RuntimeError("GET é‡è©¦å·²ç”¨ç›¡")

def http_post(url: str, *, json_body: Optional[Dict[str, Any]] = None, data: Optional[str] = None,
              headers: Optional[Dict[str, str]] = None, retries: int = MAX_RETRIES) -> requests.Response:
    for i in range(retries):
        try:
            r = session.post(url, json=json_body, data=data, headers=headers, timeout=REQUESTS_TIMEOUT)
            if r.status_code in (502, 503, 504):
                raise requests.ConnectionError(f"Transient {r.status_code}")
            return r
        except (requests.ConnectionError, requests.Timeout) as e:
            if i == retries - 1:
                raise
            log(f"POST é‡è©¦ {i+1}/{retries-1}: {e}")
            _sleep_backoff(i)
    raise RuntimeError("POST é‡è©¦å·²ç”¨ç›¡")

def _is_finite_vector(vec: Optional[List[float]], dims: int) -> bool:
    if not isinstance(vec, list) or len(vec) != dims:
        return False
    return all(isinstance(x, (int, float)) and math.isfinite(float(x)) for x in vec)

# ========== å‘é‡ç”Ÿæˆå™¨ ==========
class VectorGenerator:
    """å‘é‡ç”Ÿæˆå™¨"""
    
    def __init__(self, model: str):
        self.model = model
        if "text-embedding-3-large" in model:
            self.dimension = 3072
        else:
            self.dimension = 1536
    
    def generate(self, text: str) -> Optional[List[float]]:
        if client is None:
            log("âŒ OpenAI client æœªåˆå§‹åŒ–")
            return None
        try:
            resp = client.embeddings.create(
                model=self.model,
                input=text[:8000],
                encoding_format="float",
            )
            return resp.data[0].embedding
        except Exception as e:
            log(f"âš ï¸ å‘é‡ç”Ÿæˆå¤±æ•—ï¼š{e}")
            return None
    
    def batch_generate(self, texts: List[str]) -> List[Optional[List[float]]]:
        if client is None:
            return [None for _ in texts]
        
        # é è™•ç†
        processed: List[str] = []
        for t in texts:
            s = "" if t is None else str(t)
            s = s[:8000].strip()
            processed.append(s)
        
        # å»ºç«‹éæ¿¾å¾Œçš„ inputs
        inputs: List[str] = []
        idx_map: List[int] = []
        for i, s in enumerate(processed):
            if s:
                inputs.append(s)
                idx_map.append(i)
        
        if not inputs:
            return [None for _ in texts]
        
        try:
            resp = client.embeddings.create(
                model=self.model,
                input=inputs,
                encoding_format="float",
            )
            result: List[Optional[List[float]]] = [None for _ in texts]
            for out_vec, orig_idx in zip([d.embedding for d in resp.data], idx_map):
                result[orig_idx] = out_vec
            return result
        except Exception as e:
            log(f"âš ï¸ æ‰¹é‡ç”Ÿæˆå¤±æ•—ï¼š{e}")
            return [None for _ in texts]

# ========== ES æ›´æ–°å™¨ ==========
class ElasticsearchVectorUpdater:
    """Elasticsearch å‘é‡æ›´æ–°å™¨ - å„ªåŒ–æ–‡æœ¬æå–ç‰ˆ"""
    
    def __init__(self, vector_gen: VectorGenerator):
        self.vector_gen = vector_gen
        self.es_url = ES_URL
        self.index_pattern = INDEX_PATTERN
        self.dims = vector_gen.dimension
        self.session = requests.Session()
    
    def _list_indices(self, index_pattern: str) -> List[str]:
        try:
            r = http_get(f"{ES_URL}/_cat/indices/{index_pattern}", params={"format": "json"})
            if r.ok:
                return [row["index"] for row in r.json()]
        except Exception:
            pass
        try:
            r = http_get(f"{ES_URL}/{index_pattern}")
            if r.ok and isinstance(r.json(), dict):
                return list(r.json().keys())
        except Exception:
            pass
        return []
    
    def update_index_mapping(self, index_pattern: str = INDEX_PATTERN) -> None:
        """æ›´æ–°ç´¢å¼•æ˜ å°„ï¼Œæ·»åŠ å‘é‡æ¬„ä½"""
        mapping_update = {
            "properties": {
                "content_vector": {
                    "type": "dense_vector",
                    "dims": self.vector_gen.dimension,
                    "index": True,
                    "similarity": "cosine",
                },
                "vector_generated_at": {"type": "date"},
            }
        }
        indices = self._list_indices(index_pattern)
        if not indices:
            log(f"â„¹ï¸ æœªæ‰¾åˆ°ç¬¦åˆçš„ç´¢å¼•ï¼š{index_pattern}")
            return
        for index in indices:
            try:
                r = session.put(
                    f"{ES_URL}/{index}/_mapping",
                    json=mapping_update,
                    timeout=REQUESTS_TIMEOUT,
                )
                if r.ok:
                    log(f"âœ… å·²æ›´æ–°ç´¢å¼•æ˜ å°„ï¼š{index}")
                else:
                    log(f"âš ï¸ æ›´æ–°ç´¢å¼•æ˜ å°„å¤±æ•—ï¼š{index} {r.status_code}")
            except Exception as e:
                log(f"âš ï¸ ç´¢å¼• {index} æ˜ å°„æ›´æ–°ä¾‹å¤–ï¼š{e}")
    
    def find_documents_without_vectors(self, index_pattern: str = INDEX_PATTERN, 
                                      size: int = 100) -> List[Dict[str, Any]]:
        """æœå°‹å°šæœªå»ºç«‹ content_vector çš„æ–‡ä»¶"""
        query = {
            "size": size,
            "_source": True,
            "query": {"bool": {"must_not": [{"exists": {"field": "content_vector"}}]}},
            "sort": [{"_doc": "asc"}],
        }
        try:
            r = http_post(f"{ES_URL}/{index_pattern}/_search", json_body=query)
            if r.ok:
                body = r.json()
                hits = body.get("hits", {}).get("hits", [])
                if hits:
                    log(f"ğŸ“‹ æ‰¾åˆ° {len(hits)} å€‹æ–‡æª”éœ€è¦ç”Ÿæˆå‘é‡")
                return hits
            log(f"âš ï¸ æœå°‹å¤±æ•— {r.status_code}")
        except Exception as e:
            log(f"âš ï¸ æœå°‹ä¾‹å¤–ï¼š{e}")
        return []
    
    def _extract_text(self, source: Dict[str, Any], index_name: str) -> str:
        """æ ¹æ“šç´¢å¼•é¡å‹æå–æœ€ç›¸é—œçš„æ–‡æœ¬ - å„ªåŒ–ç‰ˆ"""
        
        # æ ¹æ“šç´¢å¼•åç¨±åˆ¤æ–·é¡å‹
        if 'ecn-notice' in index_name:
            return self._extract_ecn_notice_text(source)
        elif 'ecn-application' in index_name:
            return self._extract_ecn_application_text(source)
        elif 'complaint' in index_name:
            return self._extract_complaint_text(source)
        elif 'fmea' in index_name:
            return self._extract_fmea_text(source)
        elif 'document' in index_name:
            return self._extract_structured_document_text(source)
        else:
            return self._extract_generic_text(source)
    
    def _extract_ecn_notice_text(self, source: Dict) -> str:
        """æå–è¨­è®Šé€šçŸ¥å–®çš„é—œéµæ–‡æœ¬"""
        parts = []
        
        # å–®è™Ÿ
        if source.get('notice_number'):
            parts.append(f"è¨­è®Šé€šçŸ¥å–® {source['notice_number']}")
        
        # ç”¢å“è³‡è¨Š
        if source.get('product_name'):
            parts.append(source['product_name'])
        if source.get('product_code'):
            parts.append(f"å“è™Ÿ {source['product_code']}")
        
        # æ ¸å¿ƒå…§å®¹ï¼šè¨­è®Šèªªæ˜
        if source.get('change_description'):
            parts.append(f"è¨­è®Šèªªæ˜ï¼š{source['change_description']}")
        
        # è¨­è®Šå‰å¾Œå°æ¯”
        if source.get('before_change'):
            parts.append(f"è¨­è®Šå‰ï¼š{source['before_change']}")
        if source.get('after_change'):
            parts.append(f"è¨­è®Šå¾Œï¼š{source['after_change']}")
        
        # åº«å­˜è™•ç†
        if source.get('inventory_handling'):
            parts.append(f"åº«å­˜è™•ç†ï¼š{source['inventory_handling']}")
        
        # ç”³è«‹äºº
        if source.get('applicant'):
            parts.append(f"ç”³è«‹äººï¼š{source['applicant']}")
        
        return ' '.join(filter(None, parts))
    
    def _extract_ecn_application_text(self, source: Dict) -> str:
        """æå–è¨­è®Šç”³è«‹å–®çš„é—œéµæ–‡æœ¬"""
        parts = []
        
        # å–®è™Ÿ
        if source.get('application_number'):
            parts.append(f"è¨­è®Šç”³è«‹å–® {source['application_number']}")
        
        # ç”¢å“è³‡è¨Š
        if source.get('product_name'):
            parts.append(source['product_name'])
        if source.get('product_code'):
            parts.append(f"å“è™Ÿ {source['product_code']}")
        
        # æ ¸å¿ƒå…§å®¹ï¼šç·£ç”±
        if source.get('reason'):
            parts.append(f"ç·£ç”±ï¼š{source['reason']}")
        
        # è¨­è®Šé …ç›®
        if source.get('change_items'):
            parts.append(f"è¨­è®Šé …ç›®ï¼š{source['change_items']}")
        
        # è¨­è®Šå‰å¾Œ
        if source.get('change_before'):
            parts.append(f"è¨­è®Šå‰ï¼š{source['change_before']}")
        if source.get('change_after'):
            parts.append(f"è¨­è®Šå¾Œï¼š{source['change_after']}")
        
        # æœƒè­°å»ºè­°
        if source.get('meeting_suggestions'):
            parts.append(f"æœƒè­°å»ºè­°ï¼š{source['meeting_suggestions']}")
        
        # å¯©æŸ¥èªªæ˜
        if source.get('review_notes'):
            parts.append(f"å¯©æŸ¥èªªæ˜ï¼š{source['review_notes']}")
        
        return ' '.join(filter(None, parts))
    
    def _extract_complaint_text(self, source: Dict) -> str:
        """æå–å®¢è¨´çš„é—œéµæ–‡æœ¬"""
        parts = []
        
        # å–®è™Ÿ
        if source.get('complaint_number'):
            parts.append(f"å®¢è¨´å–® {source['complaint_number']}")
        
        # å®¢æˆ¶è³‡è¨Š
        if source.get('customer_name'):
            parts.append(f"å®¢æˆ¶ï¼š{source['customer_name']}")
        
        # ç”¢å“è³‡è¨Š
        if source.get('product_name'):
            parts.append(source['product_name'])
        if source.get('product_code'):
            parts.append(f"å“è™Ÿ {source['product_code']}")
        
        # æ ¸å¿ƒå…§å®¹ï¼šæŠ±æ€¨æè¿°
        if source.get('complaint_description'):
            parts.append(f"æŠ±æ€¨å…§å®¹ï¼š{source['complaint_description']}")
        
        # æŠ±æ€¨åˆ†æ
        if source.get('complaint_analysis'):
            parts.append(f"åˆ†æï¼š{source['complaint_analysis']}")
        
        # æ‰¿è¾¦æ¥­å‹™
        if source.get('responsible_sales'):
            parts.append(f"æ‰¿è¾¦ï¼š{source['responsible_sales']}")
        
        return ' '.join(filter(None, parts))
    
    def _extract_fmea_text(self, source: Dict) -> str:
        """æå– FMEA çš„é—œéµæ–‡æœ¬"""
        parts = []
        
        # æ¡ˆè™Ÿèˆ‡é¡å‹
        if source.get('case_number'):
            analysis_type = source.get('analysis_type', 'FMEA')
            parts.append(f"{analysis_type} {source['case_number']}")
        
        # æ¡ˆä»¶åç¨±
        if source.get('case_name'):
            parts.append(source['case_name'])
        
        # ç”¢å“åˆ¥
        if source.get('product_type'):
            parts.append(f"ç”¢å“ï¼š{source['product_type']}")
        
        # æ ¸å¿ƒå…§å®¹ï¼šåˆ†æé …ç›®
        if source.get('analysis_item'):
            parts.append(f"åˆ†æé …ç›®ï¼š{source['analysis_item']}")
        
        # å¤±æ•ˆæ¨¡å¼
        if source.get('failure_mode'):
            parts.append(f"å¤±æ•ˆæ¨¡å¼ï¼š{source['failure_mode']}")
        
        # å¤±æ•ˆå½±éŸ¿
        if source.get('failure_effect'):
            parts.append(f"å¤±æ•ˆå½±éŸ¿ï¼š{source['failure_effect']}")
        
        # å¤±æ•ˆæˆå› 
        if source.get('failure_cause'):
            parts.append(f"å¤±æ•ˆæˆå› ï¼š{source['failure_cause']}")
        
        # é¢¨éšªè©•åˆ†ï¼ˆé‡è¦æŒ‡æ¨™ï¼‰
        risk_info = []
        if source.get('severity_s'):
            risk_info.append(f"åš´é‡åº¦{source['severity_s']}")
        if source.get('occurrence_o'):
            risk_info.append(f"ç™¼ç”Ÿåº¦{source['occurrence_o']}")
        if source.get('detection_d'):
            risk_info.append(f"é›£æª¢åº¦{source['detection_d']}")
        if source.get('rpn'):
            risk_info.append(f"RPN{source['rpn']}")
        if risk_info:
            parts.append(' '.join(risk_info))
        
        # å°ç­–æ–¹æ¡ˆï¼ˆé‡è¦ï¼‰
        if source.get('corrective_action'):
            parts.append(f"å°ç­–æ–¹æ¡ˆï¼š{source['corrective_action']}")
        
        # æ”¹å–„çµæœ
        if source.get('improvement_result'):
            parts.append(f"æ”¹å–„çµæœï¼š{source['improvement_result']}")
        
        # æ˜¯å¦ç‚ºå®¢è¨´æ¡ˆ
        if source.get('is_customer_complaint'):
            parts.append("å®¢è¨´æ¡ˆ")
        
        # è² è²¬äºº
        if source.get('responsible_person'):
            parts.append(f"è² è²¬äººï¼š{source['responsible_person']}")
        
        return ' '.join(filter(None, parts))
    
    def _extract_structured_document_text(self, source: Dict) -> str:
        """æå– structured_documents çš„æ–‡æœ¬"""
        parts = []
        
        # æ–‡æª”ç·¨è™Ÿ
        if source.get('doc_number'):
            parts.append(source['doc_number'])
        
        # ç”¢å“è³‡è¨Š
        product_names = source.get('product_names')
        if isinstance(product_names, list):
            parts.extend(product_names[:3])
        elif product_names:
            parts.append(str(product_names))
        
        product_codes = source.get('product_codes')
        if isinstance(product_codes, list):
            parts.extend(product_codes[:3])
        
        # æ‘˜è¦
        if source.get('summary'):
            parts.append(source['summary'])
        
        # é—œéµå­—
        keywords = source.get('keywords')
        if isinstance(keywords, list):
            parts.extend(keywords[:5])
        
        return ' '.join(filter(None, parts))
    
    def _extract_generic_text(self, source: Dict) -> str:
        """é€šç”¨æ–‡æœ¬æå– (å‚™ç”¨)"""
        priority_fields = [
            "summary", "description", "content", "title",
            "product_name", "complaint_description", "change_description"
        ]
        
        for field in priority_fields:
            if field in source and source[field]:
                text = str(source[field])
                if len(text) > 10:
                    return text
        
        # å‚™ç”¨ï¼šæ‰€æœ‰æ–‡æœ¬æ¬„ä½
        text_parts = []
        for k, v in source.items():
            if isinstance(v, str) and len(v) > 0 and k not in ['_id', '_index']:
                text_parts.append(v)
        
        return ' '.join(text_parts[:5]) if text_parts else ""
    
    def update_document_vectors(self, docs: List[Dict[str, Any]]) -> Tuple[int, int]:
        """æ›´æ–°æ–‡æª”å‘é‡"""
        if not docs:
            return (0, 0)
        
        # æå–æ–‡æœ¬
        texts = []
        for i, d in enumerate(docs):
            source = d.get("_source", {})
            index_name = d.get("_index", "")
            text = self._extract_text(source, index_name)
            texts.append(text)
            
            # æ—¥èªŒé è¦½
            preview = text[:100].replace('\n', ' ')
            log(f"  æ–‡æª” {i+1}: {d['_id'][:8]}... ç´¢å¼•: {index_name} æ–‡æœ¬é•·åº¦: {len(text)} é è¦½: {preview}")
        
        # æ‰¹æ¬¡ç”Ÿæˆå‘é‡
        log(f"ğŸ”„ é–‹å§‹ç”Ÿæˆ {len(texts)} å€‹å‘é‡...")
        embeddings = self.vector_gen.batch_generate(texts)
        
        valid_count = sum(1 for e in embeddings if e is not None)
        log(f"  ç”Ÿæˆçµæœ: {valid_count}/{len(embeddings)} å€‹æœ‰æ•ˆå‘é‡")
        
        if valid_count == 0:
            log(f"âŒ æ‰€æœ‰å‘é‡ç”Ÿæˆå¤±æ•—ï¼")
            return (0, 0)
        
        # æº–å‚™å¯«å…¥
        doc_ids = [d["_id"] for d in docs]
        indices = [d.get("_index") for d in docs]
        dims = self.vector_gen.dimension
        
        writer = ESVectorWriter(self.es_url, index=None, field="content_vector", session=session)
        ok, ng = writer.upsert_vectors(doc_ids, indices, embeddings, dims)
        
        if ok > 0:
            log(f"âœ… æˆåŠŸå¯«å…¥ {ok} ç­†å‘é‡")
        if ng > 0:
            log(f"âŒ å¤±æ•— {ng} ç­†")
        
        return (ok, ng)

# ========== ES å‘é‡å¯«å…¥å™¨ ==========
class ESVectorWriter:
    def __init__(self, base_url: str, index: str, field: str = "content_vector",
                 session: Optional[requests.Session] = None):
        self.base_url = base_url
        self.index = index
        self.field = field
        self.session = session or requests.Session()
    
    def upsert_vectors(self, ids: List[str], indices: List[str], 
                      vectors: List[Optional[List[float]]], dims: int) -> Tuple[int, int]:
        """æ‰¹æ¬¡å¯«å…¥å‘é‡"""
        assert len(indices) == len(ids) == len(vectors)
        lines: List[str] = []
        skip_count = 0
        
        for idx, _id, vec in zip(indices, ids, vectors):
            if not idx or "*" in idx or "?" in idx:
                skip_count += 1
                continue
            if not _is_finite_vector(vec, dims):
                skip_count += 1
                continue
            
            lines.append(json.dumps({"update": {"_index": idx, "_id": _id}}))
            lines.append(json.dumps({
                "doc": {
                    self.field: vec,
                    "vector_generated_at": datetime.utcnow().isoformat()
                },
                "doc_as_upsert": True
            }))
        
        if not lines:
            return (0, skip_count)
        
        try:
            bulk_data = "\n".join(lines) + "\n"
            r = self.session.post(
                f"{self.base_url}/_bulk",
                data=bulk_data,
                headers={"Content-Type": "application/x-ndjson"},
                timeout=REQUESTS_TIMEOUT
            )
            
            if r.ok:
                result = r.json()
                success = sum(1 for item in result.get("items", []) 
                            if "error" not in item.get("update", {}))
                failed = len(lines) // 2 - success
                return (success, failed)
            else:
                log(f"âŒ bulk è«‹æ±‚å¤±æ•—: {r.status_code} - {r.text[:200]}")
                return (0, len(lines) // 2)
                
        except Exception as e:
            log(f"âŒ æ‰¹æ¬¡å¯«å…¥å¤±æ•—: {e}")
            return (0, len(lines) // 2)

# ========== ä¿¡è™Ÿè™•ç† ==========
def _handle_sigterm(signum, frame):
    global _SHOULD_STOP
    _SHOULD_STOP = True
    log("æ”¶åˆ°åœæ­¢è¨Šè™Ÿï¼Œæº–å‚™çµæŸâ€¦")

# ========== ä¸»æµç¨‹ ==========
def main() -> None:
    if not OPENAI_API_KEY:
        log("âŒ æœªè¨­ç½® OPENAI_API_KEY")
        return
    if client is None:
        log("âŒ OpenAI å¥—ä»¶æœªæ­£ç¢ºå®‰è£")
        return
    
    signal.signal(signal.SIGTERM, _handle_sigterm)
    signal.signal(signal.SIGINT, _handle_sigterm)
    
    log("=" * 60)
    log("ğŸš€ å‘é‡æœå‹™å•Ÿå‹•")
    log(f"ğŸ“Š æ¨¡å‹ï¼š{EMBEDDING_MODEL}")
    log(f"ğŸ” ç´¢å¼•æ¨¡å¼ï¼š{INDEX_PATTERN}")
    log(f"ğŸ“¦ æ‰¹æ¬¡å¤§å°ï¼š{BATCH_SIZE}")
    log(f"ğŸ¤– è‡ªå‹•åœæ­¢ï¼š{'å•Ÿç”¨' if AUTO_STOP_ENABLED else 'åœç”¨'}")
    if AUTO_STOP_ENABLED:
        log(f"   é€£çºŒç©ºè¼ªä¸Šé™ï¼š{AUTO_STOP_EMPTY_ROUNDS} æ¬¡")
    log(f"âš ï¸  å¤±æ•—åœæ­¢ä¸Šé™ï¼š{AUTO_STOP_FAIL_LIMIT} æ¬¡")
    log("=" * 60)
    
    try:
        wait_for_es()
    except Exception as e:
        log(f"âŒ ç­‰å¾… Elasticsearch å¤±æ•—ï¼š{e}")
        return
    
    vg = VectorGenerator(EMBEDDING_MODEL)
    updater = ElasticsearchVectorUpdater(vg)
    updater.update_index_mapping(INDEX_PATTERN)
    
    # è‡ªå‹•åœæ­¢è¨ˆæ•¸å™¨
    empty_rounds = 0
    consecutive_failures = 0  # é€£çºŒå¤±æ•—è¨ˆæ•¸å™¨
    total_processed = 0
    
    while not _SHOULD_STOP:
        try:
            docs = updater.find_documents_without_vectors(INDEX_PATTERN, size=BATCH_SIZE)
            if docs:
                # æ‰¾åˆ°æ–‡æª”ï¼Œé‡ç½®ç©ºè¼ªè¨ˆæ•¸å™¨
                empty_rounds = 0
                ok_count, fail_count = updater.update_document_vectors(docs)
                
                # æª¢æŸ¥æ˜¯å¦å…¨éƒ¨å¤±æ•—
                if ok_count == 0 and fail_count > 0:
                    consecutive_failures += 1
                    log(f"âš ï¸  å‘é‡ç”Ÿæˆ/å¯«å…¥å¤±æ•— (é€£çºŒå¤±æ•— {consecutive_failures}/{AUTO_STOP_FAIL_LIMIT})")
                    
                    # æª¢æŸ¥æ˜¯å¦é”åˆ°å¤±æ•—ä¸Šé™
                    if consecutive_failures >= AUTO_STOP_FAIL_LIMIT:
                        log("=" * 60)
                        log(f"âŒ éŒ¯èª¤ï¼å‘é‡æ·»åŠ é€£çºŒå¤±æ•— {consecutive_failures} æ¬¡")
                        log(f"ğŸ›‘ è‡ªå‹•åœæ­¢æœå‹™ä»¥é¿å…æŒçºŒéŒ¯èª¤")
                        log("=" * 60)
                        break
                else:
                    # æœ‰æˆåŠŸçš„ï¼Œé‡ç½®å¤±æ•—è¨ˆæ•¸å™¨
                    consecutive_failures = 0
                    total_processed += ok_count
            else:
                # æ²’æœ‰æ‰¾åˆ°æ–‡æª”
                empty_rounds += 1
                log(f"ğŸ˜´ æ‰€æœ‰æ–‡æª”éƒ½å·²æœ‰å‘é‡ (ç©ºè¼ª {empty_rounds}/{AUTO_STOP_EMPTY_ROUNDS if AUTO_STOP_ENABLED else 'âˆ'})")
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦è‡ªå‹•åœæ­¢
                if AUTO_STOP_ENABLED and empty_rounds >= AUTO_STOP_EMPTY_ROUNDS:
                    log("=" * 60)
                    log(f"âœ… å®Œæˆï¼æ‰€æœ‰æ–‡æª”éƒ½å·²æœ‰å‘é‡")
                    log(f"ğŸ“Š æœ¬æ¬¡é‹è¡Œå…±è™•ç† {total_processed} å€‹æ–‡æª”")
                    log(f"ğŸ›‘ å·²é€£çºŒ {empty_rounds} è¼ªç„¡æ–°æ–‡æª”ï¼Œè‡ªå‹•åœæ­¢æœå‹™")
                    log("=" * 60)
                    break
                    
        except Exception as e:
            consecutive_failures += 1
            log(f"âŒ ä¸»å¾ªç’°éŒ¯èª¤ (é€£çºŒå¤±æ•— {consecutive_failures}/{AUTO_STOP_FAIL_LIMIT})ï¼š{e}")
            
            # æª¢æŸ¥æ˜¯å¦é”åˆ°å¤±æ•—ä¸Šé™
            if consecutive_failures >= AUTO_STOP_FAIL_LIMIT:
                log("=" * 60)
                log(f"âŒ éŒ¯èª¤ï¼ä¸»å¾ªç’°é€£çºŒå¤±æ•— {consecutive_failures} æ¬¡")
                log(f"ğŸ›‘ è‡ªå‹•åœæ­¢æœå‹™ä»¥é¿å…æŒçºŒéŒ¯èª¤")
                log("=" * 60)
                break
        
        time.sleep(SLEEP_SEC)
    
    log("ğŸ‘‹ å‘é‡æœå‹™çµæŸ")

if __name__ == "__main__":
    main()